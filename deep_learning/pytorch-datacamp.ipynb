{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch DataCamp\n",
    "\n",
    "### Creating tensors in PyTorch\n",
    "Random tensors are very important in neural networks. Parameters of the neural networks typically are initialized with random weights (random tensors).\n",
    "\n",
    "Let us start practicing building tensors in PyTorch library. As you know, tensors are arrays with an arbitrary number of dimensions, corresponding to NumPy's ndarrays. You are going to create a random tensor of sizes 3 by 3 and set it to variable your_first_tensor. Then, you will need to print it. Finally, calculate its size in variable tensor_size and print its value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import torch\n",
    "import torch\n",
    "\n",
    "# Create random tensor of size 3 by 3\n",
    "your_first_tensor = torch.rand(3, 3)\n",
    "\n",
    "# Calculate the shape of the tensor\n",
    "tensor_size = your_first_tensor.shape\n",
    "\n",
    "# Print the values of the tensor and its shape\n",
    "print(your_first_tensor)\n",
    "print(tensor_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix multiplication\n",
    "There are many important types of matrices which have their uses in neural networks. Some important matrices are matrices of ones (where each entry is set to 1) and the identity matrix (where the diagonal is set to 1 while all other values are 0). The identity matrix is very important in linear algebra: any matrix multiplied with identity matrix is simply the original matrix.\n",
    "\n",
    "Let us experiment with these two types of matrices. You are going to build a matrix of ones with shape 3 by 3 called tensor_of_ones and an identity matrix of the same shape, called identity_tensor. We are going to see what happens when we multiply these two matrices, and what happens if we do an element-wise multiplication of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a matrix of ones with shape 3 by 3\n",
    "tensor_of_ones = torch.ones(3, 3)\n",
    "\n",
    "# Create an identity matrix with shape 3 by 3\n",
    "identity_tensor = torch.eye(3)\n",
    "\n",
    "# Do a matrix mulitplication of tensor_of_ones with identity_tensor\n",
    "matrices_multiplied = torch.matmul(tensor_of_ones, identity_tensor)\n",
    "print(matrices_multiplied)\n",
    "\n",
    "# Do an element-wise multiplication of tensor_of_ones with identity_tensor\n",
    "element_multiplication = tensor_of_ones * identity_tensor\n",
    "print(element_multiplication)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass\n",
    "Let's have something resembling more a neural network. The computational graph has been given below. You are going to initialize 3 large random tensors, and then do the operations as given in the computational graph. The final operation is the mean of the tensor, given by torch.mean(your_tensor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tensors x, y and z\n",
    "x = torch.rand(1000, 1000)\n",
    "y = torch.rand(1000, 1000)\n",
    "z = torch.rand(1000, 1000)\n",
    "\n",
    "# Multiply x with y\n",
    "q = x * y\n",
    "\n",
    "# Multiply elementwise z with q\n",
    "f = z * q\n",
    "\n",
    "mean_f = torch.mean(f)\n",
    "print(mean_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation using PyTorch\n",
    "Here, you are going to use automatic differentiation of PyTorch in order to compute the derivatives of x, y and z from the previous exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize x, y and z to values 4, -3 and 5\n",
    "x = torch.tensor(4., requires_grad=True)\n",
    "y = torch.tensor(-3., requires_grad=True)\n",
    "z = torch.tensor(5., requires_grad=True)\n",
    "\n",
    "# Set q to sum of x and y, set f to product of q with z\n",
    "q = x + y\n",
    "f = q * z\n",
    "\n",
    "# Compute the derivatives\n",
    "f.backward()\n",
    "\n",
    "# Print the gradients\n",
    "print(\"Gradient of x is: \" + str(x.grad))\n",
    "print(\"Gradient of y is: \" + str(y.grad))\n",
    "print(\"Gradient of z is: \" + str(z.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating gradients in PyTorch\n",
    "Remember the exercise in forward pass? Now that you know how to calculate derivatives, let's make a step forward and start calculating the gradients (derivatives of tensors) of the computational graph you built back then. We have already initialized for you three random tensors of shape (1000, 1000) called x, y and z. First, we multiply tensors x and y, then we do an elementwise multiplication of their product with tensor z, and then we compute its mean. In the end, we compute the derivatives.\n",
    "\n",
    "The main difference from the previous exercise is the scale of the tensors. While before, tensors x, y and z had just 1 number, now they each have 1 million numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiply tensors x and y\n",
    "q = x * y\n",
    "\n",
    "# Elementwise multiply tensors z with q\n",
    "f = z * q\n",
    "\n",
    "mean_f = torch.mean(f)\n",
    "\n",
    "# Calculate the gradients\n",
    "mean_f.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your first neural network\n",
    "You are going to build a neural network in PyTorch, using the hard way. Your input will be images of size (28, 28), so images containing 784 pixels. Your network will contain an input_layer (provided for you), a hidden layer with 200 units, and an output layer with 10 classes. The input layer has already been created for you. You are going to create the weights, and then do matrix multiplications, getting the results from the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the weights of the neural network\n",
    "weight_1 = torch.rand(784, 28)\n",
    "weight_2 = torch.rand(28, 28)\n",
    "\n",
    "# Multiply input_layer with weight_1\n",
    "hidden_1 = torch.matmul(input_layer, weight_1)\n",
    "\n",
    "# Multiply hidden_1 with weight_2\n",
    "output_layer = torch.matmul(hidden_1, weight_2)\n",
    "print(output_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your first neural network\n",
    "You are going to build a neural network in PyTorch, using the hard way. Your input will be images of size (28, 28), so images containing 784 pixels. Your network will contain an input_layer (provided for you), a hidden layer with 200 units, and an output layer with 10 classes. The input layer has already been created for you. You are going to create the weights, and then do matrix multiplications, getting the results from the network.\n",
    "\n",
    "- Initialize with random numbers two matrices of weights, called weight_1 and weight_2.\n",
    "- Set the result of input_layer times weight_1 to hidden_1. Set the result of hidden_1 times weight_2 to output_layer.\n",
    "- We need 784 input units, 200 hidden units and 10 output units.\n",
    "- You first need to multiply input_layer with weight_1, and then multiply their product with weight_2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the weights of the neural network\n",
    "weight_1 = torch.rand(784, 200)\n",
    "weight_2 = torch.rand(200, 10)\n",
    "\n",
    "# Multiply input_layer with weight_1\n",
    "hidden_1 = torch.matmul(input_layer, weight_1)\n",
    "\n",
    "# Multiply hidden_1 with weight_2\n",
    "output_layer = torch.matmul(hidden_1, weight_2)\n",
    "print(output_layer)"
   ]
  },
  {
   "source": [
    "### Your first PyTorch neural network\n",
    "You are going to build the same neural network you built in the previous exercise, but now using the PyTorch way. As a reminder, you have 784 units in the input layer, 200 hidden units and 10 units for the output layer.\n",
    "\n",
    "- Instantiate two linear layers calling them self.fc1 and self.fc2. Determine their correct dimensions.\n",
    "- Implement the .forward() method, using the two layers you defined and returning x."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Instantiate all 2 linear layers  \n",
    "        self.fc1 = nn.Linear(784, 200)\n",
    "        self.fc2 = nn.Linear(200, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "      \n",
    "        # Use the instantiated layers and return x\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "source": [
    "### Neural networks\n",
    "Let us see the differences between neural networks which apply ReLU and those which do not apply ReLU. We have already initialized the input called input_layer, and three sets of weights, called weight_1, weight_2 and weight_3.\n",
    "\n",
    "We are going to convince ourselves that networks with multiple layers which do not contain non-linearity can be expressed as neural networks with one layer.\n",
    "\n",
    "- Calculate the first and second hidden layer by multiplying the appropriate inputs with the corresponding weights.\n",
    "- Calculate and print the results of the output.\n",
    "- Set weight_composed_1 to the product of weight_1 with weight_2, then set weight to the product of weight_composed_1 with weight_3."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the first and second hidden layer\n",
    "hidden_1 = torch.matmul(input_layer, weight_1)\n",
    "hidden_2 = torch.matmul(hidden_1, weight_2)\n",
    "\n",
    "# Calculate the output\n",
    "print(torch.matmul(hidden_2, weight_3))\n",
    "\n",
    "# Calculate weight_composed_1 and weight\n",
    "weight_composed_1 = torch.matmul(weight_1, weight_2)\n",
    "weight = torch.matmul(weight_composed_1, weight_3)\n",
    "\n",
    "# Multiply input_layer with weight\n",
    "print(torch.matmul(input_layer, weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply non-linearity on hidden_1 and hidden_2\n",
    "hidden_1_activated = relu(torch.matmul(input_layer, weight_1))\n",
    "hidden_2_activated = relu(torch.matmul(hidden_1_activated, weight_2))\n",
    "print(torch.matmul(hidden_2_activated, weight_3))\n",
    "\n",
    "# Apply non-linearity in the product of first two weights. \n",
    "weight_composed_1_activated = relu(torch.matmul(weight_1, weight_2))\n",
    "\n",
    "# Multiply `weight_composed_1_activated` with `weight_3\n",
    "weight = torch.matmul(weight_composed_1_activated, weight_3)\n",
    "\n",
    "# Multiply input_layer with weight\n",
    "print(torch.matmul(input_layer, weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ReLU activation again\n",
    "Neural networks don't need to have the same number of units in each layer. Here, you are going to experiment with the ReLU activation function again, but this time we are going to have a different number of units in the layers of the neural network. The input layer will still have 4 features, but then the first hidden layer will have 6 units and the output layer will have 2 units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate ReLU activation function as relu\n",
    "relu = nn.ReLU()\n",
    "\n",
    "# Initialize weight_1 and weight_2 with random numbers\n",
    "weight_1 = torch.rand(4, 6)\n",
    "weight_2 = torch.rand(6, 2)\n",
    "\n",
    "# Multiply input_layer with weight_1\n",
    "hidden_1 = torch.matmul(input_layer, weight_1)\n",
    "\n",
    "# Apply ReLU activation function over hidden_1 and multiply with weight_2\n",
    "hidden_1_activated = relu(hidden_1)\n",
    "print(torch.matmul(hidden_1_activated, weight_2))"
   ]
  },
  {
   "source": [
    "## Loss functions\n",
    "- Initialize neural networks with random weights\n",
    "- Do a forward pass\n",
    "- Calcualte loss function (1 number)\n",
    "- Calculate the gradients\n",
    "- Change the weights based on the gradients\n",
    "- For regression: use least squared loss\n",
    "- For classification: use softmax cross-entropy\n",
    "- FOr more complicated problems (e.g. object detection): use more complicated losses\n",
    "\n",
    "### Calculating loss function in PyTorch\n",
    "You are going to code the previous exercise, and make sure that we computed the loss correctly. Predicted scores are -1.2 for class 0 (cat), 0.12 for class 1 (car) and 4.8 for class 2 (frog). The ground truth is class 2 (frog). Compute the loss function in PyTorch.\n",
    "\n",
    "- Initialize the tensor of scores with numbers [[-1.2, 0.12, 4.8]], and the tensor of ground truth [2].\n",
    "- Instantiate the cross-entropy loss and call it criterion."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the scores and ground truth\n",
    "logits = torch.tensor([[-1.2, 0.12, 4.8]])\n",
    "ground_truth = torch.tensor([2])\n",
    "\n",
    "# Instantiate cross entropy loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Compute and print the loss\n",
    "loss = criterion(logits, ground_truth)\n",
    "print(loss)"
   ]
  },
  {
   "source": [
    "### Loss function of random scores\n",
    "If the neural network predicts random scores, what would be its loss function? Let's find it out in PyTorch. The neural network is going to have 1000 classes, each having a random score. For ground truth, it will have class 111. Calculate the loss function.\n",
    "\n",
    "- Import torch and torch.nn as nn\n",
    "- Initialize logits with a random tensor of shape (1, 1000) and ground_truth with a tensor containing the number 111.\n",
    "- Instantiate the cross-entropy loss in a variable called criterion."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import torch and torch.nn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Initialize logits and ground truth\n",
    "logits = torch.rand(1, 1000)\n",
    "ground_truth = torch.tensor([111])\n",
    "\n",
    "# Instantiate cross-entropy loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Calculate and print the loss\n",
    "loss = criterion(logits, ground_truth)\n",
    "print(loss)"
   ]
  },
  {
   "source": [
    "### Preparing MNIST dataset\n",
    "You are going to prepare dataloaders for MNIST training and testing set. As we explained in the lecture, MNIST has some differences to CIFAR-10, with the main difference being that MNIST images are grayscale (1 channel based) instead of RGB (3 channels).\n",
    "\n",
    "- Transform the data to torch tensors and normalize it, mean is 0.1307 while std is 0.3081.\n",
    "- Prepare the trainset and the testset.\n",
    "- Prepare the dataloaders for training and testing so that only 32 pictures are processed at a time."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data to torch tensors and normalize it \n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "\t\t\t\t\t\t\t\ttransforms.Normalize((0.1307), ((0.3081)))])\n",
    "\n",
    "# Prepare the datasets\n",
    "trainset = torchvision.datasets.MNIST('mnist', train=True, \n",
    "\t\t\t\t\t\t\t\t\t  download=True, transform=transform)\n",
    "testset = torchvision.datasets.MNIST('mnist', train=False, \n",
    "\t\t\t\t\t\t\t\t\t  download=True, transform=transform)\n",
    "\n",
    "# Prepare the dataloaders\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32,\n",
    "\t\t\t\t\t\t\t\t\t\t  shuffle=True, num_workers=0)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=32,\n",
    "\t\t\t\t\t\t\t\t\t\t shuffle=False, num_workers=0)"
   ]
  },
  {
   "source": [
    "### Inspecting the dataloaders\n",
    "Now you are going to explore a bit the dataloaders you created in the previous exercise. In particular, you will compute the shape of the dataset in addition to the minibatch size."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the shape of the training set and testing set\n",
    "trainset_shape = trainloader.dataset.train_data.shape\n",
    "testset_shape = testloader.dataset.test_data.shape\n",
    "\n",
    "# Print the computed shapes\n",
    "print(trainset_shape, testset_shape)\n",
    "\n",
    "# Compute the size of the minibatch for training set and testing set\n",
    "trainset_batchsize = trainloader.batch_size\n",
    "testset_batchsize = testloader.batch_size\n",
    "\n",
    "# Print sizes of the minibatch\n",
    "print(trainset_batchsize, testset_batchsize)"
   ]
  },
  {
   "source": [
    "### Building a neural network - again\n",
    "You haven't created a neural network since the end of the first chapter, so this is a good time to build one (practice makes perfect). Build a class for a neural network which will be used to train on the MNIST dataset. The dataset contains images of shape (28, 28, 1), so you should deduct the size of the input layer. For hidden layer use 200 units, while for output layer use 10 units (1 for each class). For activation function, use relu in a functional way (nn.Functional is already imported as F).\n",
    "\n",
    "For context, the same net will be trained and used to make predictions in the next two exercises."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the class Net\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):    \n",
    "    \t# Define all the parameters of the net\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28 * 1, 200)\n",
    "        self.fc2 = nn.Linear(200, 10)\n",
    "\n",
    "    def forward(self, x):   \n",
    "    \t# Do the forward pass\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "source": [
    "### Training a neural network\n",
    "Given the fully connected neural network (called model) which you built in the previous exercise and a train loader called train_loader containing the MNIST dataset (which we created for you), you're to train the net in order to predict the classes of digits. You will use the Adam optimizer to optimize the network, and considering that this is a classification problem you are going to use cross entropy as loss function.\n",
    "\n",
    "- Instantiate the Adam optimizer with learning rate 3e-4 and instantiate Cross-Entropy as loss function.\n",
    "- Complete a forward pass on the neural network using the input data.\n",
    "- Using backpropagation, compute the gradients of the weights, and then change the weights using the Adam optimizer."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Adam optimizer and Cross-Entropy loss function\n",
    "model = Net()   \n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "  \n",
    "for batch_idx, data_target in enumerate(train_loader):\n",
    "    data = data_target[0]\n",
    "    target = data_target[1]\n",
    "    data = data.view(-1, 28 * 28)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Complete a forward pass\n",
    "    output = model(data)\n",
    "\n",
    "    # Compute the loss, gradients and change the weights\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "source": [
    "### Using the network to make predictions\n",
    "Now that you have trained the network, use it to make predictions for the data in the testing set. The network is called model (same as in the previous exercise), and the loader is called test_loader. We have already initialized variables total and correct to 0.\n",
    "\n",
    "- Set the network in testing (eval) mode.\n",
    "- Put each image into a vector using inputs.view(-1, number_of_features) where the number of features should be deducted by multiplying spatial dimensions (shape) of the image.\n",
    "- Do the forward pass and put the predictions in output variable."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model in eval mode\n",
    "model.eval()\n",
    "\n",
    "for i, data in enumerate(test_loader, 0):\n",
    "    inputs, labels = data\n",
    "    \n",
    "    # Put each image into a vector\n",
    "    inputs = inputs.view(-1, 28*28)\n",
    "    \n",
    "    # Do the forward pass and get the predictions\n",
    "    outputs = model(inputs)\n",
    "    _, outputs = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (outputs == labels).sum().item()\n",
    "print('The testing set accuracy of the network is: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "source": [
    "### Convolution operator - OOP way\n",
    "Let's kick off this chapter by using convolution operator from the torch.nn package. You are going to create a random tensor which will represent your image and random filters to convolve the image with. Then you'll apply those images.\n",
    "\n",
    "The torch library and the torch.nn package have already been imported for you.\n",
    "\n",
    "- Create 10 images with shape (1, 28, 28).\n",
    "- Build 6 convolutional filters of size (3, 3) with stride set to 1 and padding set to 1.\n",
    "- Apply the filters in the image and print the shape of the feature map."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 10 random images of shape (1, 28, 28)\n",
    "images = torch.rand(10, 1, 28, 28)\n",
    "\n",
    "# Build 6 conv. filters\n",
    "conv_filters = torch.nn.Conv2d(in_channels=1, out_channels=6, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "# Convolve the image with the filters \n",
    "output_feature = conv_filters(images)\n",
    "print(output_feature.shape)"
   ]
  },
  {
   "source": [
    "### Convolution operator - Functional way\n",
    "While I and most of PyTorch practitioners love the torch.nn package (OOP way), other practitioners prefer building neural network models in a more functional way, using torch.nn.functional. More importantly, it is possible to mix the concepts and use both libraries at the same time (we have already done it in the previous chapter). You are going to build the same neural network you built in the previous exercise, but this time using the functional way.\n",
    "\n",
    "As before, we have already imported the torch library and torch.nn.functional as F.\n",
    "\n",
    "- Create 10 random images with shape (1, 28, 28).\n",
    "- Create 6 random filters with shape (1, 3, 3).\n",
    "- Convolve the images with the filters."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 10 random images\n",
    "image = torch.rand(10, 1, 28, 28)\n",
    "\n",
    "# Create 6 filters\n",
    "filters = torch.rand(6, 1, 3, 3)\n",
    "\n",
    "# Convolve the image with the filters\n",
    "output_feature = F.conv2d(image, filters, stride=1, padding=1)\n",
    "print(output_feature.shape)"
   ]
  },
  {
   "source": [
    "### Max-pooling operator\n",
    "Here you are going to practice using max-pooling in both OOP and functional way, and see for yourself that the produced results are the same. We have already created and printed the image for you, and imported torch library in addition to torch.nn and torch.nn.Functional as F packages.\n",
    "\n",
    "- Build a max-pooling operator with size 2.\n",
    "- Apply the max-pooling operator in the image (loaded as im).\n",
    "- Use a max-pooling operator in functional way in the image.\n",
    "\n",
    "### Average-pooling operator\n",
    "After coding the max-pooling operator, you are now going to code the average-pooling operator. You just need to replace max-pooling with average pooling.\n",
    "\n",
    "- Build an average-pooling operator with size 2.\n",
    "- Apply the average-pooling operator in the image.\n",
    "- Use an average-pooling operator in functional way in the image, called im."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a pooling operator with size `2`.\n",
    "max_pooling = torch.nn.MaxPool2d(2)\n",
    "\n",
    "# Apply the pooling operator\n",
    "output_feature = max_pooling(im)\n",
    "\n",
    "# Use pooling operator in the image\n",
    "output_feature_F = F.max_pool2d(im, 2)\n",
    "\n",
    "# print the results of both cases\n",
    "print(output_feature)\n",
    "print(output_feature_F)\n",
    "\n",
    "# Build a pooling operator with size `2`.\n",
    "avg_pooling = torch.nn.AvgPool2d(2)\n",
    "\n",
    "# Apply the pooling operator\n",
    "output_feature = avg_pooling(im)\n",
    "\n",
    "# Use pooling operator in the image\n",
    "output_feature_F = F.avg_pool2d(im, 2)\n",
    "\n",
    "# print the results of both cases\n",
    "print(output_feature)\n",
    "print(output_feature_F)"
   ]
  },
  {
   "source": [
    "### Your first CNN - __init__ method\n",
    "You are going to build your first convolutional neural network. You're going to use the MNIST dataset as the dataset, which is made of handwritten digits from 0 to 9. The convolutional neural network is going to have 2 convolutional layers, each followed by a ReLU nonlinearity, and a fully connected layer. We have already imported torch and torch.nn as nn. Remember that each pooling layer halves both the height and the width of the image, so by using 2 pooling layers, the height and width are 1/4 of the original sizes. MNIST images have shape (1, 28, 28)\n",
    "\n",
    "For the moment, you are going to implement the __init__ method of the net. In the next exercise, you will implement the .forward() method.\n",
    "\n",
    "NB: We need 2 pooling layers, but we only need to instantiate a pooling layer once, because each pooling layer will have the same configuration. Instead, we will use self.pool twice in the next exercise.\n",
    "\n",
    "- Instantiate two convolutional filters: the first one should have 5 channels, while the second one should have 10 channels. The kernel_size for both of them should be 3, and both should use padding=1. Use the names of the arguments (instead of using 1, use padding=1).\n",
    "- Instantiate a ReLU() nonlinearity.\n",
    "- Instantiate a max pooling layer which halves the size of the image in both directions.\n",
    "- Instantiate a fully connected layer which connects the units with the number of classes (we are using MNIST, so there are 10 classes).\n",
    "\n",
    "### Your first CNN - forward() method\n",
    "Now that you have declared all the parameters of your CNN, all you need to do is to implement the net's forward() method, and voila, you have your very first PyTorch CNN.\n",
    "\n",
    "Note: for evaluation purposes, the entire code of the class needs to be in the script. We are using the __init__ method as you have coded it on the previous exercise, while you are going to code the .forward() method here.\n",
    "\n",
    "- Apply the first convolutional layer, followed by the relu nonlinearity, then in the next line apply max-pooling layer.\n",
    "- Apply the second convolutional layer, followed by the relu nonlinearity, then in the next line apply max-pooling layer.\n",
    "- Transform the feature map from 4 dimensional to 2 dimensional space. The first dimension contains the batch size (-1), deduct the second dimension, by multiplying the values for height, width and depth.\n",
    "- Apply the fully-connected layer and return the result.\n",
    "- Your image started with shape (28, 28) and on it were performed 2 pooling layers, each halving the size. Second conv layer had 10 channels. What should be the second argument of view?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "\t\t\n",
    "        # Instantiate the ReLU nonlinearity\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Instantiate two convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=5, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=5, out_channels=10, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Instantiate a max pooling layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Instantiate a fully connected layer\n",
    "        self.fc = nn.Linear(7 * 7 * 10, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "  \n",
    "        # Apply conv followd by relu, then in next line pool\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # Apply conv followd by relu, then in next line pool\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # Prepare the image for the fully connected layer\n",
    "        x = x.view(-1, 7 * 7 * 10)\n",
    "\n",
    "        # Apply the fully connected layer and return the result\n",
    "        return self.fc(x)"
   ]
  },
  {
   "source": [
    "### Training CNNs\n",
    "Similarly to what you did in Chapter 2, you are going to train a neural network. This time however, you will train the CNN you built in the previous lesson, instead of a fully connected network. The packages you need have been imported for you and the network (called net) instantiated. The cross-entropy loss function (called criterion) and the Adam optimizer (called optimizer) are also available. We have subsampled the training set so that the training goes faster, and you are going to use a single epoch.\n",
    "\n",
    "- Compute the predictions from the net.\n",
    "- Using the predictions and the labels, compute the loss function.\n",
    "- Compute the gradients for each weight.\n",
    "- Update the weights using the optimizer."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(train_loader, 0):\n",
    "    inputs, labels = data\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Compute the forward pass\n",
    "    outputs = net(inputs)\n",
    "        \n",
    "    # Compute the loss function\n",
    "    loss = criterion(outputs, labels)\n",
    "        \n",
    "    # Compute the gradients\n",
    "    loss.backward()\n",
    "        \n",
    "    # Update the weights\n",
    "    optimizer.step()"
   ]
  },
  {
   "source": [
    "### Using CNNs to make predictions\n",
    "Building and training neural networks is a very exciting job (trust me, I do it every day)! However, the main utility of neural networks is to make predictions. This is the entire reason why the field of deep learning has bloomed in the last few years, as neural networks predictions are extremely accurate. On this exercise, we are going to use the convolutional neural network you already trained in order to make predictions on the MNIST dataset.\n",
    "\n",
    "Remember that torch.max() takes two arguments: -output.data - the tensor which contains the data.\n",
    "\n",
    "Either 1 to do argmax or 0 to do max.\n",
    "\n",
    "- Iterate over the given test_loader, saving the results of each iteration in data.\n",
    "- Get the image and label from the data tuple, storing the results in image and label.\n",
    "- Make a forward pass in the net using your image.\n",
    "- Get the net prediction using torch.max() function."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the data in the test_loader\n",
    "for i, data in enumerate(test_loader):\n",
    "  \n",
    "    # Get the image and label from data\n",
    "    image, label = data\n",
    "    \n",
    "    # Make a forward pass in the net with your image\n",
    "    output = net(image)\n",
    "    \n",
    "    # Argmax the results of the net\n",
    "    _, predicted = torch.max(output.data, 1)\n",
    "    if predicted == label:\n",
    "        print(\"Yipes, your net made the right prediction \" + str(predicted))\n",
    "    else:\n",
    "        print(\"Your net prediction was \" + str(predicted) + \", but the correct label is: \" + str(label))"
   ]
  },
  {
   "source": [
    "## Sequential module\n",
    "### Sequential module - init method\n",
    "Having learned about the sequential module, now is the time to see how you can convert a neural network that doesn't use sequential modules to one that uses them. We are giving the code to build the network in the usual way, and you are going to write the code for the same network using sequential modules.\n",
    "\n",
    "We want the pooling layer to be used after the second and fourth convolutional layers, while the relu nonlinearity needs to be used after each layer except the last (fully-connected) layer. For the number of filters (kernels), stride, passing, number of channels and number of units, use the same numbers as above.\n",
    "\n",
    "### Sequential module - forward() method\n",
    "Now, that you have defined all the modules that the network needs, it is time to apply them in the forward() method. For context, we are giving the code for the forward() method, if the net was written in the usual way.\n",
    "\n",
    "Note: for evaluation purposes, the entire code of the class needs to be in the script. We are using the __init__ method as you have coded it on the previous exercise, while you are going to code the forward() method here.\n",
    "\n",
    "- Extract the features from the images.\n",
    "- Squeeze the three spatial dimensions of the feature maps into one using the view() method.\n",
    "- Classify images based on the extracted features."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Declare all the layers for feature extraction\n",
    "        self.features = nn.Sequential(nn.Conv2d(in_channels=1, out_channels=5, kernel_size=3, padding=1), \n",
    "                                      nn.ReLU(inplace=True),\n",
    "                                      nn.Conv2d(in_channels=5, out_channels=10, kernel_size=3, padding=1), \n",
    "                                      nn.MaxPool2d(2, 2), nn.ReLU(inplace=True),\n",
    "                                      nn.Conv2d(in_channels=10, out_channels=20, kernel_size=3, padding=1),\n",
    "                                      nn.ReLU(inplace=True),\n",
    "                                      nn.Conv2d(in_channels=20, out_channels=40, kernel_size=3, padding=1),\n",
    "                                      nn.MaxPool2d(2, 2), nn.ReLU(inplace=True))\n",
    "        \n",
    "        # Declare all the layers for classification\n",
    "        self.classifier = nn.Sequential(nn.Linear(7 * 7 * 40, 1024), nn.ReLU(inplace=True),\n",
    "                                       \tnn.Linear(1024, 2048), nn.ReLU(inplace=True),\n",
    "                                        nn.Linear(2048, 10))\n",
    "        \n",
    "    def forward(self, x):\n",
    "      \n",
    "        # Apply the feature extractor in the input\n",
    "        x = self.features(x)\n",
    "        \n",
    "        # Squeeze the three spatial dimensions in one\n",
    "        x = x.view(-1, 7 * 7 * 40)\n",
    "        \n",
    "        # Classify the images\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "source": [
    "### Validation set\n",
    "You saw the need for validation set in the previous video. Problem is that the datasets typically are not separated into training, validation and testing. It is your job as a data scientist to split the dataset into training, testing and validation. The easiest (and most used) way of doing so is to do a random splitting of the dataset. In PyTorch, that can be done using SubsetRandomSampler object. You are going to split the training part of MNIST dataset into training and validation. After randomly shuffling the dataset, use the first 55000 points for training, and the remaining 5000 points for validation.\n",
    "\n",
    "- Use numpy.arange() to create an array containing numbers [0, 59999] and then randomly shuffle the array.\n",
    "- In the train_loader using SubsetRandomSampler() use the first 55k points for training.\n",
    "- In the val_loader use the remaining 5k points for validation."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the indices\n",
    "indices = np.arange(60000)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Build the train loader\n",
    "train_loader = torch.utils.data.DataLoader(datasets.MNIST('mnist', download=True, train=True,\n",
    "                     transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])),\n",
    "                     batch_size=64, shuffle=False, sampler=torch.utils.data.SubsetRandomSampler(indices[:55000]))\n",
    "\n",
    "# Build the validation loader\n",
    "val_loader = torch.utils.data.DataLoader(datasets.MNIST('mnist', download=True, train=True,\n",
    "                   transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])),\n",
    "                   batch_size=64, shuffle=False, sampler=torch.utils.data.SubsetRandomSampler(indices[55000:]))"
   ]
  },
  {
   "source": [
    "## Regularization techniques\n",
    "### L2-regularization\n",
    "You are going to implement each of the regularization techniques explained in the previous video. Doing so, you will also remember important concepts studied throughout the course. You will start with l2-regularization, the most important regularization technique in machine learning. As you saw in the video, l2-regularization simply penalizes large weights, and thus enforces the network to use only small weights.\n",
    "\n",
    "- Instantiate an object called model from class Net(), which is available in your workspace (consider it as a blackbox).\n",
    "- Instantiate the cross-entropy loss.\n",
    "- Instantiate Adam optimizer with learning_rate equals to 3e-4, and l2 regularization parameter equals to 0.001."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the network\n",
    "model = Net()\n",
    "\n",
    "# Instantiate the cross-entropy loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Instantiate the Adam optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4, weight_decay=0.001)"
   ]
  },
  {
   "source": [
    "### Dropout\n",
    "You saw that dropout is an effective technique to avoid overfitting. Typically, dropout is applied in fully-connected neural networks, or in the fully-connected layers of a convolutional neural network. You are now going to implement dropout and use it on a small fully-connected neural network.\n",
    "\n",
    "For the first hidden layer use 200 units, for the second hidden layer use 500 units, and for the output layer use 10 units (one for each class). For the activation function, use ReLU. Use .Dropout() with strength 0.5, between the first and second hidden layer. Use the sequential module, with the order being: fully-connected, activation, dropout, fully-connected, activation, fully-connected.\n",
    "\n",
    "- Remember for efficiency to use inplace=True as argument in ReLU() nonlinearity\n",
    "- Don't forget that the order of operations in sequential module matters.\n",
    "- A fully connected (linear) layer takes as first argument, the number of units in the previous layer, and as second argument, the number of units in the next layer.\n",
    "- As a reminder, the number of classes is 10."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        \n",
    "        # Define all the parameters of the net\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(28*28, 200),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(200, 500),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(500, 10))\n",
    "        \n",
    "    def forward(self, x):\n",
    "    \n",
    "    \t# Do the forward pass\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Batch-normalization\n",
    "Dropout is used to regularize fully-connected layers. Batch-normalization is used to make the training of convolutional neural networks more efficient, while at the same time having regularization effects. You are going to implement the __init__ method of a small convolutional neural network, with batch-normalization. The feature extraction part of the CNN will contain the following modules (in order): convolution, max-pool, activation, batch-norm, convolution, max-pool, relu, batch-norm.\n",
    "\n",
    "The first convolutional layer will contain 10 output channels, while the second will contain 20 output channels. As always, we are going to use MNIST dataset, with images having shape (28, 28) in grayscale format (1 channel). In all cases, the size of the filter should be 3, the stride should be 1 and the padding should be 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Implement the sequential module for feature extraction\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=10, kernel_size=3, stride=1, padding=1),\n",
    "            nn.MaxPool2d(2, 2), nn.ReLU(inplace=True), nn.BatchNorm2d(10),\n",
    "            nn.Conv2d(in_channels=10, out_channels=20, kernel_size=3, stride=1, padding=1),\n",
    "            nn.MaxPool2d(2, 2), nn.ReLU(inplace=True), nn.BatchNorm2d(20))\n",
    "        \n",
    "        # Implement the fully connected layer for classification\n",
    "        self.fc = nn.Linear(in_features=7*7*20, out_features=10)"
   ]
  },
  {
   "source": [
    "## Transfer learning/Fine tuning\n",
    "### Finetuning a CNN\n",
    "Previously, you trained a model to classify handwritten digits and saved the model parameters to my_net.pth. Now you're going to classify handwritten letters, but you have a smaller training set.\n",
    "\n",
    "In the first step, you'll create a new model using this training set, but the accuracy will be poor. Next, you'll perform the same training, but you'll start with the parameters from your digit classifying model. Even though digits and letters are two different classification problems, you'll see that using information from your previous model will dramatically improve this one."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model\n",
    "model = Net()\n",
    "\n",
    "# Load the parameters from the old model\n",
    "model.load_state_dict(torch.load('my_net.pth'))\n",
    "\n",
    "# Change the number of out channels\n",
    "model.fc = nn.Linear(7 * 7 * 512, 26)\n",
    "\n",
    "# Train and evaluate the model\n",
    "model.train()\n",
    "train_net(model, optimizer, criterion)\n",
    "print(\"Accuracy of the net is: \" + str(model.eval()))"
   ]
  },
  {
   "source": [
    "### Torchvision module\n",
    "You already finetuned a net you had pretrained. In practice though, it is very common to finetune CNNs that someone else (typically the library's developers) have pretrained in ImageNet. Big networks still take a lot of time to be trained on large datasets, and maybe you cannot afford to train a large network on a dataset of 1.2 million images on your laptop.\n",
    "\n",
    "Instead, you can simply download the network and finetune it on your dataset. That's what you will do right now. You are going to assume that you have a personal dataset, containing the images from all your last 7 holidays. You want to build a neural network that can classify each image depending on the holiday it comes from. However, since the dataset is so small, you need to use the finetuning technique.\n",
    "\n",
    "- Import the module that lets you download state-of-the-art CNNs.\n",
    "- Download and load a pretrained ResNet18 network.\n",
    "- Freeze all the layers bar the final one.\n",
    "- Change the last layer to correspond to the number of classes (7) in your dataset."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the module\n",
    "import torchvision\n",
    "\n",
    "# Download resnet18\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "# Freeze all the layers bar the last one\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Change the number of output units\n",
    "model.fc = nn.Linear(512, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}