{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Time Series Analysis in Python\n",
    "\n",
    "### A \"Thin\" Application of Time Series\n",
    "Google Trends allows users to see how often a term is searched for. We downloaded a file from Google Trends containing the frequency over time for the search word \"diet\", which is pre-loaded in a DataFrame called diet. A first step when analyzing a time series is to visualize the data with a plot. You should be able to clearly see a gradual decrease in searches for \"diet\" throughout the calendar year, hitting a low around the December holidays, followed by a spike in searches around the new year as people make New Year's resolutions to lose weight.\n",
    "\n",
    "Like many time series datasets you will be working with, the index of dates are strings and should be converted to a datetime index before plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas and plotting modules\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert the date index to datetime\n",
    "diet.index = pd.to_datetime(diet.index)\n",
    "\n",
    "# Plot 2012 data using slicing\n",
    "diet['2012'].plot()\n",
    "plt.show()\n",
    "\n",
    "# Plot the entire time series diet and show gridlines\n",
    "diet.plot(grid=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging Time Series With Different Dates\n",
    "\n",
    "Stock and bond markets in the U.S. are closed on different days. For example, although the bond market is closed on Columbus Day (around Oct 12) and Veterans Day (around Nov 11), the stock market is open on those days. One way to see the dates that the stock market is open and the bond market is closed is to convert both indexes of dates into sets and take the difference in sets.\n",
    "\n",
    "The pandas .join() method is a convenient tool to merge the stock and bond DataFrames on dates when both markets are open.\n",
    "\n",
    "Stock prices and 10-year US Government bond yields, which were downloaded from FRED, are pre-loaded in DataFrames stocks and bonds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Convert the stock index and bond index into sets\n",
    "set_stock_dates = set(stocks.index)\n",
    "set_bond_dates = set(bonds.index)\n",
    "\n",
    "# Take the difference between the sets and print\n",
    "print(set_stock_dates - set_bond_dates)\n",
    "\n",
    "# Merge stocks and bonds DataFrames using join()\n",
    "stocks_and_bonds = stocks.join(bonds, how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation of Stocks and Bonds\n",
    "\n",
    "Investors are often interested in the correlation between the returns of two different assets for asset allocation and hedging purposes. In this exercise, you'll try to answer the question of whether stocks are positively or negatively correlated with bonds. Scatter plots are also useful for visualizing the correlation between the two variables.\n",
    "\n",
    "Keep in mind that you should compute the correlations on the percentage changes rather than the levels.\n",
    "\n",
    "Stock prices and 10-year bond yields are combined in a DataFrame called stocks_and_bonds under columns SP500 and US10Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute percent change using pct_change()\n",
    "returns = stocks_and_bonds.pct_change()\n",
    "\n",
    "# Compute correlation using corr()\n",
    "correlation = returns['SP500'].corr(returns['US10Y'])\n",
    "print(\"Correlation of stocks and interest rates: \", correlation)\n",
    "\n",
    "# Make scatter plot\n",
    "plt.scatter(returns['SP500'], returns['US10Y'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flying Saucers Aren't Correlated to Flying Markets\n",
    "\n",
    "Two trending series may show a strong correlation even if they are completely unrelated. This is referred to as \"spurious correlation\". That's why when you look at the correlation of say, two stocks, you should look at the correlation of their returns and not their levels.\n",
    "\n",
    "To illustrate this point, calculate the correlation between the levels of the stock market and the annual sightings of UFOs. Both of those time series have trended up over the last several decades, and the correlation of their levels is very high. Then calculate the correlation of their percent changes. This will be close to zero, since there is no relationship betweem those two series.\n",
    "\n",
    "The DataFrame levels contains the levels of DJI and UFO. UFO data was downloaded from www.nuforc.org."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation of levels\n",
    "correlation1 = levels['DJI'].corr(levels['UFO'])\n",
    "print(\"Correlation of levels: \", correlation1)\n",
    "\n",
    "# Compute correlation of percent changes\n",
    "changes = levels.pct_change()\n",
    "correlation2 = changes['DJI'].corr(changes['UFO'])\n",
    "print(\"Correlation of changes: \", correlation2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at a Regression's R-Squared\n",
    "\n",
    "R-squared measures how closely the data fit the regression line, so the R-squared in a simple regression is related to the correlation between the two variables. In particular, the magnitude of the correlation is the square root of the R-squared and the sign of the correlation is the sign of the regression coefficient.\n",
    "\n",
    "In this exercise, you will start using the statistical package statsmodels, which performs much of the statistical modeling and testing that is found in R and software packages like SAS and MATLAB.\n",
    "\n",
    "You will take two series, x and y, compute their correlation, and then regress y on x using the function OLS() in the statsmodels.api library. Most linear regressions contain a constant term which is the intercept (the α in the regression yt=α+βxt+ϵt). To include a constant using the function OLS(), you need to add a column of 1's to the right hand side of the regression.\n",
    "\n",
    "The module statsmodels.api has been imported for you as sm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the statsmodels module\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Compute correlation of x and y\n",
    "correlation = x.corr(y)\n",
    "print(\"The correlation between x and y is %4.2f\" %(correlation))\n",
    "\n",
    "# Convert the Series x to a DataFrame and name the column x\n",
    "x = pd.DataFrame(x, columns=['x'])\n",
    "\n",
    "# Add a constant to the DataFrame x\n",
    "x = sm.add_constant(x)\n",
    "\n",
    "# Fit the regression of y on x\n",
    "result = sm.OLS(y, x).fit()\n",
    "\n",
    "# Print out the results and look at the relationship between R-squared and the correlation above\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autocorrelation\n",
    "- Correlation of a time series with a lagged version of itself\n",
    "- e.g. with dates lag-one correlation means that it looks at the previous day\n",
    "\n",
    "### A Popular Strategy Using Autocorrelation\n",
    "One puzzling anomaly with stocks is that investors tend to overreact to news. Following large jumps, either up or down, stock prices tend to reverse. This is described as mean reversion in stock prices: prices tend to bounce back, or revert, towards previous levels after large moves, which are observed over time horizons of about a week. A more mathematical way to describe mean reversion is to say that stock returns are negatively autocorrelated.\n",
    "\n",
    "This simple idea is actually the basis for a popular hedge fund strategy. If you're curious to learn more about this hedge fund strategy (although it's not necessary reading for anything else later in the course), see here.\n",
    "\n",
    "You'll look at the autocorrelation of weekly returns of MSFT stock from 2012 to 2017. You'll start with a DataFrame MSFT of daily prices. You should use the .resample() method to get weekly prices and then compute returns from prices. Use the pandas method .autocorr() to get the autocorrelation and show that the autocorrelation is negative. Note that the .autocorr() method only works on Series, not DataFrames (even DataFrames with one column), so you will have to select the column in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the daily data to weekly data\n",
    "MSFT = MSFT.resample(rule='W', how='last')\n",
    "\n",
    "# Compute the percentage change of prices\n",
    "returns = MSFT.pct_change()\n",
    "\n",
    "# Compute and print the autocorrelation of returns\n",
    "autocorrelation = returns['Adj Close'].autocorr()\n",
    "print(\"The autocorrelation of weekly returns is %4.2f\" %(autocorrelation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are Interest Rates Autocorrelated?\n",
    "\n",
    "When you look at daily changes in interest rates, the autocorrelation is close to zero. However, if you resample the data and look at annual changes, the autocorrelation is negative. This implies that while short term changes in interest rates may be uncorrelated, long term changes in interest rates are negatively autocorrelated. A daily move up or down in interest rates is unlikely to tell you anything about interest rates tomorrow, but a move in interest rates over a year can tell you something about where interest rates are going over the next year. And this makes some economic sense: over long horizons, when interest rates go up, the economy tends to slow down, which consequently causes interest rates to fall, and vice versa.\n",
    "\n",
    "The DataFrame daily_data contains daily data of 10-year interest rates from 1962 to 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the daily change in interest rates \n",
    "daily_data['change_rates'] = daily_data.diff()\n",
    "\n",
    "# Compute and print the autocorrelation of daily changes\n",
    "autocorrelation_daily = daily_data['change_rates'].autocorr()\n",
    "print(\"The autocorrelation of daily interest rate changes is %4.2f\" %(autocorrelation_daily))\n",
    "\n",
    "# Convert the daily data to annual data\n",
    "annual_data = daily_data['US10Y'].resample(rule='A', how='last')\n",
    "\n",
    "# Repeat above for annual data\n",
    "annual_data['diff_rates'] = annual_data.diff()\n",
    "autocorrelation_annual = annual_data['diff_rates'].autocorr()\n",
    "print(\"The autocorrelation of annual interest rate changes is %4.2f\" %(autocorrelation_annual))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taxing Exercise: Compute and plot the ACF\n",
    "\n",
    "In the last chapter, you computed autocorrelations with one lag. Often we are interested in seeing the autocorrelation over many lags. The quarterly earnings for H&R Block (ticker symbol HRB) is plotted on the right, and you can see the extreme cyclicality of its earnings. A vast majority of its earnings occurs in the quarter that taxes are due.\n",
    "\n",
    "You will compute the array of autocorrelations for the H&R Block quarterly earnings that is pre-loaded in the DataFrame HRB. Then, plot the autocorrelation function using the plot_acf module. This plot shows what the autocorrelation function looks like for cyclical earnings data. The ACF at lag=0 is always one, of course. In the next exercise, you will learn about the confidence interval for the ACF, but for now, supress the confidence interval by setting alpha=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the acf module and the plot_acf module from statsmodels\n",
    "from statsmodels.tsa.stattools import acf\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "# Compute the acf array of HRB\n",
    "acf_array = acf(HRB)\n",
    "print(acf_array)\n",
    "\n",
    "# Plot the acf function\n",
    "# alpha=1 supresses CIs\n",
    "plot_acf(HRB, alpha=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are We Confident This Stock is Mean Reverting?\n",
    "In the last chapter, you saw that the autocorelation of MSFT's weekly stock returns was -0.16. That autocorrelation seems large, but is it statistically significant? In other words, can you say that there is less than a 5% chance that we would observe such a large negative autocorrelation if the true autocorrelation were really zero? And are there any autocorrelations at other lags that are significantly different from zero?\n",
    "\n",
    "Even if the true autocorrelations were zero at all lags, in a finite sample of returns you won't see the estimate of the autocorrelations exactly zero. In fact, the standard deviation of the sample autocorrelation is 1/N‾‾√ where N is the number of observations, so if N=100, for example, the standard deviation of the ACF is 0.1, and since 95% of a normal curve is between +1.96 and -1.96 standard deviations from the mean, the 95% confidence interval is ±1.96/N‾‾√. This approximation only holds when the true autocorrelations are all zero.\n",
    "\n",
    "You will compute the actual and approximate confidence interval for the ACF, and compare it to the lag-one autocorrelation of -0.16 from the last chapter. The weekly returns of Microsodt is pre-loaded in a DataFrame called returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the plot_acf module from statsmodels and sqrt from math\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from math import sqrt\n",
    "\n",
    "# Compute and print the autocorrelation of MSFT weekly returns\n",
    "autocorrelation = returns['Adj Close'].autocorr()\n",
    "print(\"The autocorrelation of weekly MSFT returns is %4.2f\" %(autocorrelation))\n",
    "\n",
    "# Find the number of observations by taking the length of the returns DataFrame\n",
    "nobs = len(returns)\n",
    "\n",
    "# Compute the approximate confidence interval\n",
    "conf = 1.96/sqrt(nobs)\n",
    "print(\"The approximate confidence interval is +/- %4.2f\" %(conf))\n",
    "\n",
    "# Plot the autocorrelation function with 95% confidence intervals and 20 lags using plot_acf\n",
    "plot_acf(returns, alpha=0.05, lags=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can't Forecast White Noise\n",
    "A white noise time series is simply a sequence of uncorrelated random variables that are identically distributed. Stock returns are often modelled as white noise. Unfortunately, for white noise, we cannot forecast future observations based on the past - autocorrelations at all lags are zero.\n",
    "\n",
    "You will generate a white noise series and plot the autocorrelation function to show that it is zero for all lags. You can use np.random.normal() to generate random returns. For a Gaussian white noise process, the mean and standard deviation describe the entire process.\n",
    "\n",
    "Plot this white noise series to see what it looks like, and then plot the autocorrelation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the plot_acf module from statsmodels\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "# Simulate wite noise returns\n",
    "# loc=mean, scale=std\n",
    "returns = np.random.normal(loc=0.02, scale=0.05, size=1000)\n",
    "\n",
    "# Print out the mean and standard deviation of returns\n",
    "mean = np.mean(returns)\n",
    "std = np.std(returns)\n",
    "print(\"The mean is %5.3f and the standard deviation is %5.3f\" %(mean,std))\n",
    "\n",
    "# Plot returns series\n",
    "plt.plot(returns)\n",
    "plt.show()\n",
    "\n",
    "# Plot autocorrelation function of white noise returns\n",
    "plot_acf(returns, lags=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random walk\n",
    "- Today's price = yesterday's price + noise\n",
    "- Can't forecast a random walk: best guess is the current value\n",
    "- Random walk with drift, prices will drift by a certain average at each point\n",
    "- Test if a series like prices follows a random walk:\n",
    "    - Regress current prices on lag prices\n",
    "    - Use the slope cofficient Beta\n",
    "        - H0: B = 1 (random walk)\n",
    "        - H1: B < 1 (not random walk)\n",
    "    - Alternative way: regress difference in prices on the lag price\n",
    "    - Instead of testing whether the slope coefficient is 1, now test if its 0\n",
    "    - Called the Dickey-Fuller test\n",
    "    - If you add more lagged changes on the right side = Augmented Dickey-Fuller Test\n",
    "    - Statsmodels has function for implementing this test: ADFuller\n",
    "    - If p < 0.05 with this test, can reject null hypothesis that it is a random walk\n",
    "        - H0: B = 0 (random walk)\n",
    "        - H1: B < 0 (not random walk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a Random Walk\n",
    "Whereas stock returns are often modelled as white noise, stock prices closely follow a random walk. In other words, today's price is yesterday's price plus some random noise.\n",
    "\n",
    "You will simulate the price of a stock over time that has a starting price of 100 and every day goes up or down by a random amount. Then, plot the simulated stock price. If you hit the \"Run Code\" code button multiple times, you'll see several realizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 500 random steps with mean=0 and standard deviation=1\n",
    "steps = np.random.normal(loc=0, scale=1, size=500)\n",
    "\n",
    "# Set first element to 0 so that the first price will be the starting stock price\n",
    "steps[0]=0\n",
    "\n",
    "# Simulate stock prices, P with a starting price of 100\n",
    "P = 100 + np.cumsum(steps)\n",
    "\n",
    "# Plot the simulated stock prices\n",
    "plt.plot(P)\n",
    "plt.title(\"Simulated Random Walk\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Drift\n",
    "In the last exercise, you simulated stock prices that follow a random walk. You will extend this in two ways in this exercise.\n",
    "\n",
    "You will look at a random walk with a drift. Many time series, like stock prices, are random walks but tend to drift up over time.\n",
    "In the last exercise, the noise in the random walk was additive: random, normal changes in price were added to the last price. However, when adding noise, you could theoretically get negative prices. Now you will make the noise multiplicative: you will add one to the random, normal changes to get a total return, and multiply that by the last price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 500 random steps\n",
    "steps = np.random.normal(loc=0.001, scale=0.01, size=500) + 1\n",
    "\n",
    "# Set first element to 1\n",
    "steps[0]=1\n",
    "\n",
    "# Simulate the stock price, P, by taking the cumulative product\n",
    "P = 100 * np.cumprod(steps)\n",
    "\n",
    "# Plot the simulated stock prices\n",
    "plt.plot(P)\n",
    "plt.title(\"Simulated Random Walk with Drift\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are Stock Prices a Random Walk?\n",
    "Most stock prices follow a random walk (perhaps with a drift). You will look at a time series of Amazon stock prices, pre-loaded in the DataFrame AMZN, and run the 'Augmented Dickey-Fuller Test' from the statsmodels library to show that it does indeed follow a random walk.\n",
    "\n",
    "With the ADF test, the \"null hypothesis\" (the hypothesis that we either reject or fail to reject) is that the series follows a random walk. Therefore, a low p-value (say less than 5%) means we can reject the null hypothesis that the series is a random walk.\n",
    "\n",
    "If the p value is very small, we can reject the hypothesis that the returns are a random walk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the adfuller module from statsmodels\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Run the ADF test on the price series and print out the results\n",
    "results = adfuller(AMZN['Adj Close'])\n",
    "print(results)\n",
    "\n",
    "# Just print out the p-value\n",
    "# results[0] is the test statistic\n",
    "print('The p-value of the test on prices is: ' + str(results[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the adfuller module from statsmodels\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Create a DataFrame of AMZN returns\n",
    "AMZN_ret = AMZN.pct_change()\n",
    "\n",
    "# Eliminate the NaN in the first row of returns\n",
    "AMZN_ret = AMZN_ret.dropna()\n",
    "\n",
    "# Run the ADF test on the return series and print out the p-value\n",
    "results = adfuller(AMZN_ret['Adj Close'])\n",
    "print('The p-value of the test on returns is: ' + str(results[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stationarity\n",
    "- Different definitions\n",
    "- Strong stationarity: entire distribition of the data is time-invariant\n",
    "- Weak stationary: mean, variance and autocorrelation are time-invariant\n",
    "- A non-stationary process is difficult to model\n",
    "    - Parameters vary with time\n",
    "    - Too many parameters to estimate\n",
    "    - Could end up with more parameters than data\n",
    "    - Stationarity is necessary for a parsimonious model: enough data to estimate\n",
    "    - Random walk is a common type of non-stationary series where variance grows with time\n",
    "    - Seasonal series are also non-stationary\n",
    "    - White noise is a stationary process unless the mean changes over time\n",
    "    - Many non-stationary series can be made stationary by taking the differences\n",
    "        - Random walk: take 1st differences\n",
    "        - Quarterly seasonal variation: take lag of 4 differences\n",
    "        - Sometimes you need to make 2 transformations e.g. Amazon\n",
    "            - Exponential growth: take log of series\n",
    "            - Seasonal difference: 4th lag diff\n",
    "            \n",
    "### Seasonal Adjustment During Tax Season\n",
    "\n",
    "Many time series exhibit strong seasonal behavior. The procedure for removing the seasonal component of a time series is called seasonal adjustment. For example, most economic data published by the government is seasonally adjusted.\n",
    "\n",
    "You saw earlier that by taking first differences of a random walk, you get a stationary white noise process. For seasonal adjustments, instead of taking first differences, you will take differences with a lag corresponding to the periodicity.\n",
    "\n",
    "Look again at the ACF of H&R Block's quarterly earnings, pre-loaded in the DataFrame HRB, and there is a clear seasonal component. The autocorrelation is high for lags 4,8,12,16,..., because of the spike in earnings every four quarters during tax season. Apply a seasonal adjustment by taking the fourth difference (four represents the periodicity of the series). Then compute the autocorrelation of the transformed series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the plot_acf module from statsmodels\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "# Seasonally adjust quarterly earnings\n",
    "HRBsa = HRB.diff(4)\n",
    "\n",
    "# Print the first 10 rows of the seasonally adjusted series\n",
    "print(HRBsa.head(10))\n",
    "\n",
    "# Drop the NaN data in the first three three rows\n",
    "HRBsa = HRBsa.dropna()\n",
    "\n",
    "# Plot the autocorrelation function of the seasonally adjusted series\n",
    "plot_acf(HRBsa)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AR Model\n",
    "\n",
    "- Autoregressive model\n",
    "- Today's value = mean + fraction of yesterday's value (phi) + noise\n",
    "- If only one lagged value on the right of the equation:\n",
    "    - model of order one\n",
    "    - AR(1) model\n",
    "- If phi is 1: process = random walk\n",
    "- If phi is 0: process = white noise\n",
    "- Therefore phi must be between -1 and +1\n",
    "- Negative phi means mean reversal\n",
    "    - If for a stock that had positive return at time t-1\n",
    "    - Now is predicted to have negative return\n",
    "- Positive phi means positive result\n",
    "    - Stock will have positive return in this period\n",
    "- Autocorrelation decays exponentially at a rate of phi\n",
    "- Therefore, if phi = 0.9:\n",
    "    - Lage 1 autocorrelation = 0.9\n",
    "    - Lag 2 autocorrelation = 0.9^2\n",
    "    - Lag 3 autocorrelation = 0.9^3\n",
    "    - etc\n",
    "- If phi is negative, it still decays exponentially, the signs of the autocorrelation function reverse at each lag\n",
    "\n",
    "### Simulate AR(1) Time Series\n",
    "You will simulate and plot a few AR(1) time series, each with a different parameter, ϕ, using the arima_process module in statsmodels. In this exercise, you will look at an AR(1) model with a large positive ϕ and a large negative ϕ, but feel free to play around with your own parameters.\n",
    "\n",
    "There are a few conventions when using the arima_process module that require some explanation. First, these routines were made very generally to handle both AR and MA models. We will cover MA models next, so for now, just ignore the MA part. Second, when inputting the coefficients, you must include the zero-lag coefficient of 1, and the sign of the other coefficients is opposite what we have been using (to be consistent with the time series literature in signal processing). For example, for an AR(1) process with ϕ=0.9, the array representing the AR parameters would be `ar = np.array([1, -0.9])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the module for simulating data\n",
    "from statsmodels.tsa.arima_process import ArmaProcess\n",
    "\n",
    "# Plot 1: AR parameter = +0.9\n",
    "plt.subplot(2,1,1)\n",
    "ar1 = np.array([1, -0.9])\n",
    "#  For now, the MA parmater array, ma1, will contain just the lag-zero coefficient of one\n",
    "ma1 = np.array([1])\n",
    "AR_object1 = ArmaProcess(ar1, ma1)\n",
    "simulated_data_1 = AR_object1.generate_sample(nsample=1000)\n",
    "plt.plot(simulated_data_1)\n",
    "\n",
    "# Plot 2: AR parameter = -0.9\n",
    "plt.subplot(2,1,2)n\n",
    "ar2 = np.array([1, 0.9])\n",
    "ma2 = np.array([1])\n",
    "AR_object2 = ArmaProcess(ar2, ma2)\n",
    "simulated_data_2 = AR_object2.generate_sample(nsample=1000)\n",
    "plt.plot(simulated_data_2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the ACF for Several AR Time Series\n",
    "\n",
    "The autocorrelation function decays exponentially for an AR time series at a rate of the AR parameter. For example, if the AR parameter, ϕ=+0.9, the first-lag autocorrelation will be 0.9, the second-lag will be (0.9)2=0.81, the third-lag will be (0.9)3=0.729, etc. A smaller AR parameter will have a steeper decay, and for a negative AR parameter, say -0.9, the decay will flip signs, so the first-lag autocorrelation will be -0.9, the second-lag will be (−0.9)2=0.81, the third-lag will be (−0.9)3=−0.729, etc.\n",
    "\n",
    "The object simulated_data_1 is the simulated time series with an AR parameter of +0.9, simulated_data_2 is for an AR paramter of -0.9, and simulated_data_3 is for an AR parameter of 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the plot_acf module from statsmodels\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "# Plot 1: AR parameter = +0.9\n",
    "plot_acf(simulated_data_1, alpha=1, lags=20)\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: AR parameter = -0.9\n",
    "plot_acf(simulated_data_2, alpha=1, lags=20)\n",
    "plt.show()\n",
    "\n",
    "# Plot 3: AR parameter = +0.3\n",
    "plot_acf(simulated_data_3, alpha=1, lags=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
