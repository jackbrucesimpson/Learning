{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programmers Guide to Data Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering\n",
    "Type of recommendation system based on the preferences of other people who are similar to you.\n",
    "\n",
    "![](../resources/euclidian_distance.jpeg)\n",
    "\n",
    "$c = \\sqrt{a^2 + b^2}$\n",
    "\n",
    "For more than one book, create a matrix of their ratings (users are columns, rows are books). You can then calculate the difference between two people and also the squared difference. By finding the square root of the summed squared differences you can find the euclidian distance. \n",
    "\n",
    "$\\sqrt{\\sum (x_i-y_i)^2}$\n",
    "\n",
    "If someone has only reviewed a couple of the same books as another person, then you can get strange results. You need a decent sample size in common."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def manhattan_distance(x1, y1, x2, y2):\n",
    "    distance = abs(x1-x2) + abs(y1-y2)\n",
    "    return distance\n",
    "\n",
    "def euclidian_distance(x1, y1, x2, y2):\n",
    "    x_diff = (x1-x2)\n",
    "    y_diff = (y1-y2)\n",
    "    sum_squared_diffs = math.pow(x_diff, 2) + math.pow(y_diff, 2)\n",
    "    distance = math.sqrt(sum_squared_diffs)\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pearson Correlation\n",
    "\n",
    "People can rate things differently - for instance someone may like everything and rate it all 4-5 while someone may avoid extremes and rate 2-4. To get around this, you can use Pearson correlation coefficient. If you plot the scores of two people, then if you get a straight line then you can show that they are in agreement. Alternatively, if they're all over the place it means they don't agree. The coefficient ranges between -1 to 1: 1 means perfect agreement, -1 means perfect disagreement.\n",
    "\n",
    "$$r = \\frac{\\sum x_iy_i - \\frac{\\sum_{i=1}^{n} x_i \\sum y_i}{n}}\n",
    "{\\sqrt{x_i^2 - \\frac{(\\sum x_i)^2}{n}} \\sqrt{y_i^2 - \\frac{(\\sum y_i)^2}{n}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pearson_cor(person1_ratings, person2_ratings):\n",
    "    def numerator\n",
    "\n",
    "\n",
    "\n",
    "    summed_ratings_multiplied = 0\n",
    "    sum_person1_ratings = 0\n",
    "    sum_person2_ratings = 0\n",
    "    for i, score in enumerate(person1_ratings):\n",
    "        summed_ratings_multiplied += person1_ratings[i] * person2_ratings[i]\n",
    "\n",
    "        sum_person1_ratings += person1_ratings[i]\n",
    "        sum_person2_ratings += person2_ratings[i]\n",
    "\n",
    "    summed_totals_multiplied_divided_n = sum_person1_ratings * sum_person2_ratings / len(person1_ratings)\n",
    "\n",
    "    numerator = summed_ratings_multiplied - summed_totals_multiplied_divided_n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
