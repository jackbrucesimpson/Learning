{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extreme Gradient Boosting with XGBoost DataCamp\n",
    "## Supervised Learning\n",
    "- Classification\n",
    "    - Outcome can be binary or multi-class\n",
    "    - Area under the ROC Curve (AUC) most common metric\n",
    "    - AUC: Probability randomly chosen positive datapoint will have a higher rank than a randomly chosen negative datapoint\n",
    "    - Higher AUC = more sensitive, better performing model\n",
    "    - With multi-class problems, look at the accuracy score and use confusion matrices\n",
    "- Regression\n",
    "    - Continuous value prediction\n",
    "    - Metrics: Root Mean Squared Error (RMSE) or or mean absolute error MAE\n",
    "    - RMSE: Difference between predicted and actual, squaring differences, computing mean, then taking square root. Enables you to treat negative and positive differences equally, but punishes larger differences more than smaller ones\n",
    "    MAE: Sums actual differences between all samples. Isn't as affected by big differences, but not as popular as RMSE\n",
    "- Data\n",
    "    - Features numeric or categorical\n",
    "    - Features scaled (e.g. Z scores): essential for algorithms like SVM\n",
    "    - Categorical features should be encoded (one-hot)\n",
    "- Problems other than classification and regression\n",
    "    - Ranking\n",
    "    - Recommendation\n",
    "\n",
    "## XGBoost\n",
    "\n",
    "- Speed and performance are the reasons it has become popular\n",
    "- Outperforms other single-algorithm methods: state-of-the-art results\n",
    "- Has cross-validation baked in (don't have to use scikit-learn)\n",
    "- Why use XGBoost\n",
    "    - Large number of trainning samples e.g. > 1000 with < 100 features\n",
    "    - Mixture of categorical and numeric, or just numeric features\n",
    "- Why not use XGBoost\n",
    "    - Images, computer vision, NLP, etc do better with neural networks\n",
    "    - Very few training samples e.g. < 100 samples or samples are significantly fewer than number of features\n",
    "\n",
    "### Example: ride-sharing data\n",
    "Data from a ride-sharing app with user behaviors over their first month of app usage in a set of imaginary cities as well as whether they used the service 5 months after sign-up. It has been pre-loaded for you into a DataFrame called churn_data.\n",
    "\n",
    "Your goal is to use the first month's worth of data to predict whether the app's users will remain users of the service at the 5 month mark. This is a typical setup for a churn prediction problem. To do this, you'll split the data into training and test sets, fit a small xgboost model on the training set, and evaluate its performance on the test set by computing its accuracy.\n",
    "\n",
    "```\n",
    "avg_dist                       float64\n",
    "avg_rating_by_driver           float64\n",
    "avg_rating_of_driver           float64\n",
    "avg_inc_price                  float64\n",
    "inc_pct                        float64\n",
    "weekday_pct                    float64\n",
    "fancy_car_user                    bool\n",
    "city_Carthag                     int64\n",
    "city_Harko                       int64\n",
    "phone_iPhone                     int64\n",
    "first_month_cat_more_1_trip      int64\n",
    "first_month_cat_no_trips         int64\n",
    "month_5_still_here               int64\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Create arrays for the features and the target: X, y\n",
    "X, y = churn_data.iloc[:,:-1], churn_data.iloc[:,-1]\n",
    "\n",
    "# Create the training and test sets\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Instantiate the XGBClassifier: xg_cl\n",
    "xg_cl = xgb.XGBClassifier(objective='binary:logistic', n_estimators=10, seed=123)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "xg_cl.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set: preds\n",
    "preds = xg_cl.predict(X_test)\n",
    "\n",
    "# Compute the accuracy: accuracy\n",
    "accuracy = float(np.sum(preds==y_test))/y_test.shape[0]\n",
    "print(\"accuracy: %f\" % (accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision trees\n",
    "- Decision trees are the base learners of XGBoost\n",
    "- Base learner: individual learning algorithm in an ensemble algorithm\n",
    "- XGBoot is an ensemble method that uses the outputs of many simple models to make its predictions\n",
    "- Decision trees are composed of a series of binary decisions that split the data and yield a prediction\n",
    "- Predictions happen at the 'leaves' of the tree\n",
    "- Decision trees are constructed iteratively (one binary decision at a time) until a stopping criterion is met (e.g. the depth of the tree reaches a particular value\n",
    "- Low bias: Good at learning relationships\n",
    "- High variance: Tend to overfit data you train them on and generalise poorly\n",
    "- XGBoost uses a type of decision tree called CART (Classification and Regression Tree)\n",
    "- CART: each leaf contains a real value school regardless of whether it is used for classification or regression\n",
    "- Cart: Real value score can be converted to categories for classification problems if necessary\n",
    "\n",
    "### Make a decision tree\n",
    "Your task in this exercise is to make a simple decision tree using scikit-learn's DecisionTreeClassifier on the breast cancer dataset that comes pre-loaded with scikit-learn.\n",
    "\n",
    "This dataset contains numeric measurements of various dimensions of individual tumors (such as perimeter and texture) from breast biopsies and a single outcome value (the tumor is either malignant, or benign)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create the training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Instantiate the classifier: dt_clf_4\n",
    "# specify max num successive split points before reaching a leaf node\n",
    "dt_clf_4 = DecisionTreeClassifier(max_depth=4)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "dt_clf_4.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set: y_pred_4\n",
    "y_pred_4 = dt_clf_4.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the predictions: accuracy\n",
    "accuracy = float(np.sum(y_pred_4==y_test))/y_test.shape[0]\n",
    "print(\"accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting\n",
    "- Ensemble meta-algorithm applied to convert many weak-learners (model with performance > 50%) into a strong learner\n",
    "- Iteratively learning a set of weak models on subsets of the data\n",
    "- Weighing each weak prediction according to each weak learner's performance\n",
    "- Combine all weak learners predictions multiplied by their weights to get a single weighted prediction\n",
    "- Combined base learners create a final predictor that is non-linear\n",
    "- Want base learners than are slightly better than guessing on certain subsets of training examples and uniformly bad at the remainder so that when they're combined the uniformly bad predictions cancel out and those that do slightly better than chance combine into a good prediction\n",
    "\n",
    "You'll now practice using XGBoost's learning API through its baked in cross-validation capabilities. As Sergey discussed in the previous video, XGBoost gets its lauded performance and efficiency gains by utilizing its own optimized data structure for datasets called a DMatrix.\n",
    "\n",
    "In the previous exercise, the input datasets were converted into DMatrix data on the fly, but when you use the xgboost cv object, you have to first explicitly convert your data into a DMatrix. So, that's what you will do here before running cross-validation on churn_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DMatrix: churn_dmatrix\n",
    "churn_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"objective\":\"reg:logistic\", \"max_depth\":3}\n",
    "\n",
    "# Perform cross-validation: cv_results\n",
    "# num_boosting_rounds is the number of trees we want to build\n",
    "# folds is the number of cross-validation folds\n",
    "# as_pandas return as pandas df\n",
    "cv_results = xgb.cv(dtrain=churn_dmatrix, params=params, nfold=3, num_boost_round=5, metrics=\"error\", as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)\n",
    "\n",
    "# Print the accuracy\n",
    "print(((1-cv_results[\"test-error-mean\"]).iloc[-1]))\n",
    "\n",
    "# measuring AUC\n",
    "cv_results = xgb.cv(dtrain=churn_dmatrix, params=params, nfold=3, num_boost_round=5, metrics=\"auc\", as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)\n",
    "\n",
    "# Print the AUC\n",
    "print((cv_results[\"test-auc-mean\"]).iloc[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective (loss) functions\n",
    "- Quantifies how far prediction is from actual result\n",
    "- Measures distance between between estimates and true values for a collection of data\n",
    "- Find the model that minimises the loss function\n",
    "- Loss functions in XGBoost\n",
    "    - `reg:linear` for regression problems\n",
    "    - `reg:logistic` for binary classification problems when you just want the decision, not the probability\n",
    "    - `binary:logistic` when you want the probability rather than the decision\n",
    "    \n",
    "## Regression with XGBoost\n",
    "\n",
    "### Decision trees as base learners\n",
    "It's now time to build an XGBoost model to predict house prices - not in Boston, Massachusetts, as you saw in the video, but in Ames, Iowa! This dataset of housing prices has been pre-loaded into a DataFrame called df. If you explore it in the Shell, you'll see that there are a variety of features about the house and its location in the city.\n",
    "\n",
    "In this exercise, your goal is to use trees as base learners. By default, XGBoost uses trees as base learners, so you don't have to specify that you want to use trees here with booster=\"gbtree\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Instantiate the XGBRegressor: xg_reg\n",
    "xg_reg = xgb.XGBRegressor(objective='reg:linear', n_estimators=10, seed=123)\n",
    "\n",
    "# Fit the regressor to the training set\n",
    "xg_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set: preds\n",
    "preds = xg_reg.predict(X_test)\n",
    "\n",
    "# Compute the rmse: rmse\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear base learners\n",
    "Now that you've used trees as base models in XGBoost, let's use the other kind of base model that can be used with XGBoost - a linear learner. This model, although not as commonly used in XGBoost, allows you to create a regularized linear regression using XGBoost's powerful learning API. However, because it's uncommon, you have to use XGBoost's own non-scikit-learn compatible functions to build the model, such as xgb.train().\n",
    "\n",
    "In order to do this you must create the parameter dictionary that describes the kind of booster you want to use (similarly to how you created the dictionary in Chapter 1 when you used xgb.cv()). The key-value pair that defines the booster type (base model) you need is \"booster\":\"gblinear\".\n",
    "\n",
    "Once you've created the model, you can use the .fit() and .predict() methods of the model just like you've done in the past."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
