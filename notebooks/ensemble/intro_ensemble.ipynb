{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Models\n",
    "- Combining diverse set of learners (individual models) together to improvise on the stability and predictive power of the model.\n",
    "- 3 broad model types\n",
    "    - Bagging (Bootstrap Aggregation): building multiple models (usually same type) from different subsamples of training data.\n",
    "    - Boosting: building multiple models (usually same type) sequentially which learn to fix errors of prior model\n",
    "    - Voting: building multiple models (usually different types) and simple statistics (e.g. mean) used to combine predictions\n",
    "\n",
    "## Decision Trees\n",
    "- Classification and Regression Trees (CART)\n",
    "- Advantages\n",
    "    - Easy to understand\n",
    "    - Useful during data exploration\n",
    "    - Less data cleaning required\n",
    "    - Data type not a constraint: handles categorical & numerical\n",
    "    - Non parametric\n",
    "- Disadvantages:\n",
    "    - Overfitting\n",
    "- Types:\n",
    "    - Categorical Variable Decision Tree: categorical target variable\n",
    "    - Continuous Variable Decision Tree: continuous target variable\n",
    "- Does not require special data preparation\n",
    "- Decision trees are sensitive to the specific data on which they are trained. If the training data is changed (e.g. a tree is trained on a subset of the training data) the resulting decision tree can be quite different and in turn the predictions can be quite different.\n",
    "- Foundation for algorithms: bagged decision trees, random forest and boosted decision trees.\n",
    "- Representation for the CART model is a binary tree\n",
    "- Creating a CART model involves selecting input variables and split points on those variables until a suitable tree is constructed.\n",
    "- Creating a binary decision tree:\n",
    "    - Process of dividing up the input space. \n",
    "    - A greedy approach is used to divide the space called recursive binary splitting.\n",
    "    - All the values are lined up and different split points are tried and tested using a cost function. \n",
    "    - The split with the best cost (lowest cost because we minimize cost) is selected.\n",
    "    - For regression predictive modeling problems the cost function that is minimized to choose split points is the sum squared error across all training samples that fall within the rectangle: sum(y – prediction)^2\n",
    "    - For classification the Gini index function is used which provides an indication of how “pure” the leaf nodes are (how mixed the training data assigned to each node is). G = sum(pk * (1 – pk)) Where G is the Gini index over all classes, pk are the proportion of training instances with class k in the rectangle of interest. A node that has all classes of the same type (perfect class purity) will have G=0, where as a G that has a 50-50 split of classes for a binary classification problem (worst purity) will have a G=0.5.\n",
    "- Stopping Criterion\n",
    "    - The recursive binary splitting procedure described above needs to know when to stop splitting as it works its way down the tree with the training data.\n",
    "    - Most common stopping procedure is to use a minimum count on the number of training instances assigned to each leaf node. If the count is less than some minimum then the split is not accepted and the node is taken as a final leaf node.\n",
    "    - The count of training members is tuned to the dataset, e.g. 5 or 10. It defines how specific to the training data the tree will be. Too specific (e.g. a count of 1) and the tree will overfit the training data and likely have poor performance on the test set.\n",
    "- Pruning the tree\n",
    "    - The stopping criterion is important as it strongly influences the performance of your tree. You can use pruning after learning your tree to further lift performance.\n",
    "    - The complexity of a decision tree is defined as the number of splits in the tree. Simpler trees are preferred. They are easy to understand (you can print them out and show them to subject matter experts), and they are less likely to overfit your data.\n",
    "    - The fastest and simplest pruning method is to work through each leaf node in the tree and evaluate the effect of removing it using a hold-out test set. Leaf nodes are removed only if it results in a drop in the overall cost function on the entire test set. You stop removing nodes when no further improvements can be made.\n",
    "    - More sophisticated pruning methods can be used such as cost complexity pruning (also called weakest link pruning) where a learning parameter (alpha) is used to weigh whether nodes can be removed based on the size of the sub-tree.\n",
    "\n",
    "## Bagging\n",
    "- Implement similar learners on small sample populations and then takes a mean of all the predictions.\n",
    "\n",
    "### Bagged Decision Trees\n",
    "- Bagging works best for algorithms with high variance like decision trees (which are often constructed without pruning)\n",
    "- Example: use the BaggingClassifier with the Classification and Regression Trees algorithm (DecisionTreeClassifier) to create a total of 100 trees\n",
    "- Running the example, we get a robust estimate of model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagged Decision Trees for Classification\n",
    "import pandas\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "seed = 7\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "cart = DecisionTreeClassifier()\n",
    "num_trees = 100\n",
    "model = BaggingClassifier(base_estimator=cart, n_estimators=num_trees, random_state=seed)\n",
    "results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "- Random forest is an extension & improvement of bagged decision trees.\n",
    "- Samples of the training dataset are taken with replacement\n",
    "- Trees are constructed in a way that reduces the correlation between individual classifiers. \n",
    "- Rather than choosing the best split point in the construction of the tree, only a random subset of features are considered for each split.\n",
    "- Builds multiple decision trees and amalgamates them together to get a more accurate and stable prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classification\n",
    "import pandas\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "seed = 7\n",
    "num_trees = 100\n",
    "max_features = 3\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "model = RandomForestClassifier(n_estimators=num_trees, max_features=max_features)\n",
    "results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random forest parameters\n",
    "- Improve predictive power:\n",
    "    - max_features\n",
    "        - Max number of features Random Forest is allowed to try in individual tree. There are multiple options.\n",
    "        - Auto/None : This will simply take all the features which make sense in every tree.Here we simply do not put any restrictions on the individual tree.\n",
    "        - sqrt : This option will take square root of the total number of features in individual run. For instance, if the total number of variables are 100, we can only take 10 of them in individual tree. 'log2' is another similar type of option for max_features.\n",
    "        - 0.2 : This option allows the random forest to take 20% of variables in individual run. We can assign and value in a format “0.x” where we want x% of features to be considered.\n",
    "        - Increasing max_features generally improves the performance of the model as at each node now we have a higher number of options to be considered. However, this is not necessarily true as this decreases the diversity of individual tree which is the USP of random forest. But, for sure, you decrease the speed of algorithm by increasing the max_features.\n",
    "    - n_estimators\n",
    "        - number of trees you want to build before taking the maximum voting or averages of predictions\n",
    "        - Higher number of trees give you better performance but makes your code slower.\n",
    "        - Choose as high value as your processor can handle because this makes your predictions stronger and more stable.\n",
    "    - min_sample_leaf\n",
    "        - Leaf is the end node of a decision tree. A smaller leaf makes the model more prone to capturing noise in train data.\n",
    "        - I prefer a minimum leaf size of more than 50. However, you should try multiple leaf sizes to find the most optimum for your use case.\n",
    "- Training speed\n",
    "    - n_jobs\n",
    "        -  How many processors is it allowed to use.\n",
    "        - A value of “-1” means there is no restriction whereas a value of “1” means it can only use one processor.\n",
    "    - random_state\n",
    "        - Makes a solution easy to replicate\n",
    "        - A definite value of random_state will always produce same results if given with same parameters and training data.\n",
    "        - I have personally found an ensemble with multiple models of different random states and all optimum parameters sometime performs better than individual random state.\n",
    "    - oob_score\n",
    "        - Random forest cross validation method.\n",
    "        - Similar but faster than leave one out validation.\n",
    "        - This method simply tags every observation used in different tress & finds out a maximum vote score for every observation based on only trees which did not use this particular observation to train itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas as pd\n",
    "\n",
    "x = pd.read_csv(\"train.csv\")\n",
    "y = x.pop(\"Survived\")\n",
    "model =  RandomForestRegressor(n_estimator = 100 , oob_score = TRUE, random_state = 42)\n",
    "model.fit(x(numeric_variable,y))\n",
    "print(\"AUC - ROC : \", roc_auc_score(y,model.oob_prediction))\n",
    "\n",
    "# Tuning model\n",
    "\n",
    "sample_leaf_options = [1,5,10,50,100,200,500]\n",
    "for leaf_size in sample_leaf_options:\n",
    "    model = RandomForestRegressor(n_estimator = 200, oob_score = TRUE, n_jobs = -1,random_state =50, max_features = \"auto\", min_samples_leaf = leaf_size)\n",
    "    model.fit(x(numeric_variable,y))\n",
    "    print(\"AUC - ROC : \", roc_auc_score(y,model.oob_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Trees\n",
    "- Extra Trees are another modification of bagging where random trees are constructed from samples of the training dataset.\n",
    "- The example below provides a demonstration of extra trees with the number of trees set to 100 and splits chosen from 7 random features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra Trees Classification\n",
    "import pandas\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "seed = 7\n",
    "num_trees = 100\n",
    "max_features = 7\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "model = ExtraTreesClassifier(n_estimators=num_trees, max_features=max_features)\n",
    "results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "- Boosting ensemble algorithms creates a sequence of models that attempt to correct the mistakes of the models before them in the sequence.\n",
    "- Once created, the models make predictions which may be weighted by their demonstrated accuracy and the results are combined to create a final output prediction.\n",
    "- Common boosting algorithms: AdaBoost & Stochastic Gradient Boosting\n",
    "- Adjust the weight of an observation based on the last classification. If an observation was classified incorrectly, it tries to increase the weight of this observation and vice versa. Boosting in general decreases the bias error and builds strong predictive models. However, they may sometimes over fit on the training data.\n",
    "\n",
    "### AdaBoost\n",
    "- Works by weighting instances in the dataset by how easy or difficult they are to classify, allowing the algorithm to pay or or less attention to them in the construction of subsequent models.\n",
    "- The example below demonstrates the construction of 30 decision trees in sequence using the AdaBoost algorithm.\n",
    "- AdaBoost is best used to boost the performance of decision trees on binary classification problems.\n",
    "- Used for classification rather than regression.\n",
    "- AdaBoost can be used to boost the performance of any machine learning algorithm. It is best used with weak learners.\n",
    "- The most suited and therefore most common algorithm used with AdaBoost are decision trees with one level. Because these trees are so short and only contain one decision for classification, they are often called decision stumps.\n",
    "- Weak models are added sequentially, trained using the weighted training data\n",
    "- Data preparation:\n",
    "    - Quality Data: Ensemble method continues to attempt to correct misclassifications in the training data\n",
    "    - Outliers: Force the ensemble down the rabbit hole of working hard to correct for cases that are unrealistic - remove\n",
    "    - Noisy Data: Especially noise in the output variable can cause problems - try to isolate and clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost Classification\n",
    "import pandas\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "seed = 7\n",
    "num_trees = 30\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "model = AdaBoostClassifier(n_estimators=num_trees, random_state=seed)\n",
    "results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Boosting (also called Gradient Boosting Machines)\n",
    "- Sophisticated ensemble technique: one of the the best available for improving performance via ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic Gradient Boosting Classification\n",
    "import pandas\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "seed = 7\n",
    "num_trees = 100\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "model = GradientBoostingClassifier(n_estimators=num_trees, random_state=seed)\n",
    "results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting/Stacking\n",
    "- Voting is one of the simplest ways of combining the predictions from multiple machine learning algorithms.\n",
    "- Use a learner to combine output from different learners.\n",
    "- It works by first creating two or more standalone models from your training dataset. A Voting Classifier can then be used to wrap your models and average the predictions of the sub-models when asked to make predictions for new data.\n",
    "- The predictions of the sub-models can be weighted, but specifying the weights for classifiers manually or even heuristically is difficult. More advanced methods can learn how to best weight the predictions from submodels, but this is called stacking (stacked aggregation) and is currently not provided in scikit-learn.\n",
    "- The code below provides an example of combining the predictions of logistic regression, classification and regression trees and support vector machines together for a classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voting Ensemble for Classification\n",
    "import pandas\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "seed = 7\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "# create the sub models\n",
    "estimators = []\n",
    "model1 = LogisticRegression()\n",
    "estimators.append(('logistic', model1))\n",
    "model2 = DecisionTreeClassifier()\n",
    "estimators.append(('cart', model2))\n",
    "model3 = SVC()\n",
    "estimators.append(('svm', model3))\n",
    "# create the ensemble model\n",
    "ensemble = VotingClassifier(estimators)\n",
    "results = model_selection.cross_val_score(ensemble, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
