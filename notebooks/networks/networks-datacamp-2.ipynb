{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Networks Datacamp 2\n",
    "\n",
    "### Exploratory data analysis\n",
    "Here, a graph G has been loaded into your workspace. Your task is to perform some exploratory analysis in the IPython Shell. What is the type of this graph? How many nodes and edges are present?\n",
    "\n",
    "The .nodes() and .edges() methods will be useful to you here, as will the type() and len() functions. Using these, you can start describing the graph's basic statistics.\n",
    "\n",
    "### Plotting using nxviz\n",
    "Now, you're going to practice creating a CircosPlot using nxviz! As a bonus preview of what's coming up in the next video, there's a little segment on the bipartite keyword in this exercise!\n",
    "\n",
    "Here, the degree centrality score of each node has been added to their metadata dictionary for you using the following code:\n",
    "\n",
    "```\n",
    "# Add the degree centrality score of each node to their metadata dictionary\n",
    "dcs = nx.degree_centrality(G)\n",
    "for n in G.nodes():\n",
    "    G.node[n]['centrality'] = dcs[n]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the CircosPlot object: c\n",
    "c = CircosPlot(G, node_color='bipartite', node_grouping='bipartite', node_order='centrality')\n",
    "\n",
    "# Draw c to the screen\n",
    "c.draw()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bipartite graphs\n",
    "- Graphs partitioned into 2 sets\n",
    "- Nodes can only be connected to nodes in other partition\n",
    "- Opposite of unipartite graphs\n",
    "- Networkx does not have an explicit bipartite graph, but you can filter nodes to get the same effect\n",
    "\n",
    "### Bipartite keywork\n",
    "In the video, Eric introduced you to the 'bipartite' keyword. This keyword is part of a node's metadata dictionary, and can be assigned both when you add a node and after the node is added. Remember, though, that by definition, in a bipartite graph, a node cannot be connected to another node in the same partition.\n",
    "\n",
    "Here, you're going to write a function that returns the nodes from a given partition in a bipartite graph. In this case, the relevant partitions of the Github bipartite graph you'll be working with are 'projects' and 'users'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define get_nodes_from_partition()\n",
    "def get_nodes_from_partition(G, partition):\n",
    "    # Initialize an empty list for nodes to be returned\n",
    "    nodes = []\n",
    "    # Iterate over each node in the graph G\n",
    "    for n in G.nodes():\n",
    "        # Check that the node belongs to the particular partition\n",
    "        if G.node[n]['bipartite'] == partition:\n",
    "            # If so, append it to the list of nodes\n",
    "            nodes.append(n)\n",
    "    return nodes\n",
    "\n",
    "# Print the number of nodes in the 'projects' partition\n",
    "print(len(get_nodes_from_partition(G, 'projects')))\n",
    "\n",
    "# Print the number of nodes in the 'users' partition\n",
    "print(len(get_nodes_from_partition(G, 'users')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degree centrality distribution of user nodes\n",
    "\n",
    "In this exercise and the next one, you're going to do a final recap of material from the previous course. Your task is to plot the degree centrality distributions for each node partition in the bipartite version of the GitHub collaboration network. Here, you'll do this for the 'users' partition. In the next exercise, you'll do this for the 'projects' partition.\n",
    "\n",
    "The function you wrote before, get_nodes_from_partition(), has been loaded for you. Just to remind you, the \"degree centrality\" is a measure of node importance, and the \"degree centrality distribution\" is the list of degree centrality scores for all nodes in the graph. A few exercises ago, when you made the circos plot, we computed the degree centralities for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the 'users' nodes: user_nodes\n",
    "user_nodes = get_nodes_from_partition(G, 'users')\n",
    "\n",
    "# Compute the degree centralities: dcs\n",
    "dcs = nx.degree_centrality(G)\n",
    "\n",
    "# Get the degree centralities for user_nodes: user_dcs\n",
    "user_dcs = [dcs[n] for n in user_nodes]\n",
    "\n",
    "# Plot the degree distribution of users_dcs\n",
    "plt.yscale('log')\n",
    "plt.hist(user_dcs, bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degree centrality distribution of project nodes\n",
    "Now it's time to plot the degree cenrality distribution for the 'projects' partition of G. The steps to do this are exactly the same as in the previous exercise. For your convenience, matplotlib.pyplot has been pre-imported as plt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get the 'projects' nodes: project_nodes\n",
    "project_nodes = get_nodes_from_partition(G, 'projects')\n",
    "\n",
    "# Compute the degree centralities: dcs\n",
    "dcs = nx.degree_centrality(G)\n",
    "\n",
    "# Get the degree centralities for project_nodes: project_dcs\n",
    "project_dcs = [dcs[n] for n in project_nodes]\n",
    "\n",
    "# Plot the degree distribution of project_dcs\n",
    "plt.yscale('log')\n",
    "plt.hist(project_dcs, bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared nodes in other partition\n",
    "In order to build up your concept of recommendation systems, we are going to start with the fundamentals. The focus here is on computing user similarity in bipartite graphs.\n",
    "\n",
    "Your job is to write a function that takes in two nodes, and returns the set of repository nodes that are shared between the two user nodes.\n",
    "\n",
    "You'll find the following methods and functions helpful in this exercise - .neighbors(), set(), and .intersection() - besides, of course, the shared_partition_nodes function that you will define."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shared_partition_nodes(G, node1, node2):\n",
    "    # Check that the nodes belong to the same partition\n",
    "    assert G.node[node1]['bipartite'] == G.node[node2]['bipartite']\n",
    "\n",
    "    # Get neighbors of node 1: nbrs1\n",
    "    nbrs1 = G.neighbors(node1)\n",
    "    # Get neighbors of node 2: nbrs2\n",
    "    nbrs2 = G.neighbors(node2)\n",
    "\n",
    "    # Compute the overlap using set intersections\n",
    "    overlap = set(nbrs1).intersection(nbrs2)\n",
    "    return overlap\n",
    "\n",
    "# Print the number of shared repositories between users 'u7909' and 'u2148'\n",
    "print(len(shared_partition_nodes(G, 'u7909', 'u2148')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User similarity metric\n",
    "Having written a function to calculate the set of nodes that are shared between two nodes, you're now going to write a function to compute a metric of similarity between two users: the number of projects shared between two users divided by the total number of nodes in the other partition. This can then be used to find users that are similar to one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_similarity(G, user1, user2, proj_nodes):\n",
    "    # Check that the nodes belong to the 'users' partition\n",
    "    assert G.node[user1]['bipartite'] == 'users'\n",
    "    assert G.node[user2]['bipartite'] == 'users'\n",
    "\n",
    "    # Get the set of nodes shared between the two users\n",
    "    shared_nodes = shared_partition_nodes(G, user1, user2)\n",
    "\n",
    "    # Return the fraction of nodes in the projects partition\n",
    "    return len(shared_nodes) / len(get_nodes_from_partition(G, 'projects'))\n",
    "\n",
    "# Compute the similarity score between users 'u4560' and 'u1880'\n",
    "project_nodes = get_nodes_from_partition(G, 'projects')\n",
    "similarity_score = user_similarity(G, 'u4560', 'u1880', project_nodes)\n",
    "\n",
    "print(similarity_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find similar users\n",
    "You're now going to build upon what you've learned so far to write a function called most_similar_users() that finds the users most similar to another given user.\n",
    "\n",
    "The beginnings of this function have been written for you. A list of nodes, user_nodes has been created, which contains all of the users except the given user that has been passed into the function. Your task is to complete the function such that it finds the users most similar to this given user. You'll make use of your user_similarity() function from the previous exercise to help do this.\n",
    "\n",
    "A dictionary called similarities has been setup, in which the keys are the scores and the list of values are the nodes. If you've never seen a defaultdict before, don't worry - you'll learn more about it in Chapter 3! It functions exactly like a regular Python dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def most_similar_users(G, user, user_nodes, proj_nodes):\n",
    "    # Data checks\n",
    "    assert G.node[user]['bipartite'] == 'users'\n",
    "\n",
    "    # Get other nodes from user partition\n",
    "    user_nodes = set(user_nodes)\n",
    "    user_nodes.remove(user)\n",
    "\n",
    "    # Create the dictionary: similarities\n",
    "    similarities = defaultdict(list)\n",
    "    for n in user_nodes:\n",
    "        similarity = user_similarity(G, user, n, proj_nodes)\n",
    "        similarities[similarity].append(n)\n",
    "\n",
    "    # Compute maximum similarity score: max_similarity\n",
    "    max_similarity = max(similarities.keys())\n",
    "\n",
    "    # Return list of users that share maximal similarity\n",
    "    return similarities[max_similarity]\n",
    "\n",
    "user_nodes = get_nodes_from_partition(G, 'users')\n",
    "project_nodes = get_nodes_from_partition(G, 'projects')\n",
    "\n",
    "print(most_similar_users(G, 'u4560', user_nodes, project_nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommend repositories\n",
    "You're close to the end! Here, the task is to practice using set differences, and you'll apply it to recommending repositories from a second user that the first user should contribute to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_repositories(G, from_user, to_user):\n",
    "    # Get the set of repositories that from_user has contributed to\n",
    "    from_repos = set(G.neighbors(from_user))\n",
    "    # Get the set of repositories that to_user has contributed to\n",
    "    to_repos = set(G.neighbors(to_user))\n",
    "\n",
    "    # Identify repositories that the from_user is connected to that the to_user is not connected to\n",
    "    return from_repos.difference(to_repos)\n",
    "\n",
    "# Print the repositories to be recommended\n",
    "print(recommend_repositories(G, 'u7909', 'u2148'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projection\n",
    "- Projected graph gets turns a bipartite graph into a unipartite graph\n",
    "- Shows you only connections from each segmented graph\n",
    "\n",
    "### Reading graphs\n",
    "In this exercise, before you compute projections, you're going to practice working with one of NetworkX's disk I/O functions, read_edgelist(). read_edgelist() creates a graph from the edgelist file. The graph that you'll be working with is a bipartite graph describing the American Revolution. There are two node partitions - 'people' and 'clubs', and edges denote a person being a member of a club."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import networkx\n",
    "import networkx as nx\n",
    "\n",
    "# Read in the data: g\n",
    "G = nx.read_edgelist('american-revolution.edgelist')\n",
    "\n",
    "# Assign nodes to 'clubs' or 'people' partitions\n",
    "# In the dataset, 'clubs' do not have a . symbol in their node name\n",
    "for n, d in G.nodes(data=True):\n",
    "    if '.' in n:\n",
    "        G.node[n]['bipartite'] = 'people'\n",
    "    else:\n",
    "         G.node[n]['bipartite'] = 'clubs'\n",
    "        \n",
    "# Print the edges of the graph\n",
    "print(G.edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing projection\n",
    "It's now time to to try your hand at computing the projection of a bipartite graph to the nodes on one of its partitions. This will help you gain practice with converting between a bipartite version of a graph and its unipartite projections. Remember from the video that the \"projection\" of a graph onto one of its partitions is the connectivity of the nodes in that partition conditioned on connections to nodes on the other partition. Made more concretely, you can think of the \"connectivity of customers based on shared purchases\".\n",
    "\n",
    "To help you get started, here's a hint on list comprehensions. List comprehensions can include conditions, so if you want to filter a graph for a certain type of node, you can do: [n for n, d in G.nodes(data=True) if d['key'] == 'some_value']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the nodelists needed for computing projections: people, clubs\n",
    "# 2 ways to do it\n",
    "people = [n for n in G.nodes() if G.node[n]['bipartite'] == 'people']\n",
    "clubs = [n for n, d in G.nodes(data=True) if d['bipartite'] == 'clubs']\n",
    "\n",
    "# Compute the people and clubs projections: peopleG, clubsG\n",
    "peopleG = nx.bipartite.projected_graph(G, people)\n",
    "clubsG = nx.bipartite.projected_graph(G, clubs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot degree centrality on projection\n",
    "Here, you're going to compare the degree centrality distributions for each of the following graphs: the original graph G, the people graph projection peopleG, and the clubs graph projection clubsG. This will reinforce the difference in degree centrality score computation between bipartite and unipartite versions of degree centrality metrics. The node lists people and clubs have been pre-loaded for you.\n",
    "\n",
    "Recall from the video that the bipartite functions require passing in a container of nodes, but will return all degree centrality scores nonetheless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the degree centrality distribution of both node partitions from the original graph\n",
    "plt.figure() \n",
    "original_dc = nx.bipartite.degree_centrality(G, people)  \n",
    "plt.hist(list(original_dc.values()), alpha=0.5)\n",
    "plt.yscale('log')\n",
    "plt.title('Bipartite degree centrality')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot the degree centrality distribution of the peopleG graph\n",
    "plt.figure()\n",
    "people_dc = nx.degree_centrality(peopleG)\n",
    "plt.hist(list(people_dc.values()))\n",
    "plt.yscale('log')\n",
    "plt.title('Degree centrality of people partition')\n",
    "plt.show()\n",
    "\n",
    "# Plot the degree centrality distribution of the clubsG graph\n",
    "plt.figure()\n",
    "clubs_dc = nx.degree_centrality(clubsG)\n",
    "plt.hist(list(clubs_dc.values()))\n",
    "plt.yscale('log')\n",
    "plt.title('Degree centrality of clubs partition')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of bipartite adjacency matrices.\n",
    "- Bipartite graph adjacency matrices are often asymmetric because there are more ways to construct an asymmetric bipartite graph than a symmetric one.\n",
    "\n",
    "### Compute adjacency matrix\n",
    "Now, you'll get some practice using matrices and sparse matrix multiplication to compute projections! In this exercise, you'll use the matrix multiplication operator @ that was introduced in Python 3.5.\n",
    "\n",
    "You'll continue working with the American Revolution graph. The two partitions of interest here are 'people' and 'clubs'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of people and list of clubs from the graph: people_nodes, clubs_nodes\n",
    "people_nodes = get_nodes_from_partition(G, 'people')\n",
    "clubs_nodes = get_nodes_from_partition(G, 'clubs')\n",
    "\n",
    "# Compute the biadjacency matrix: bi_matrix\n",
    "bi_matrix = nx.bipartite.biadjacency_matrix(G, row_order=people_nodes, column_order=clubs_nodes)\n",
    "\n",
    "# Compute the user-user projection: user_matrix\n",
    "user_matrix = bi_matrix @ bi_matrix.T\n",
    "\n",
    "print(user_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find shared membership: Transposition\n",
    "As you may have observed, you lose the metadata from a graph when you go to a sparse matrix representation. You're now going to learn how to impute the metadata back so that you can learn more about shared membership.\n",
    "\n",
    "The user_matrix you computed in the previous exercise has been preloaded into your workspace.\n",
    "\n",
    "Here, the np.where() function will prove useful. This is what it does: given an array, say, a = [1, 5, 9, 5], if you want to get the indices where the value is equal to 5, you can use idxs = np.where(a == 5). This gives you back an array in a tuple, (array([1, 3]),). To access those indices, you would want to index into the tuple as idxs[0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Find out the names of people who were members of the most number of clubs\n",
    "diag = user_matrix.diagonal()\n",
    "indices = np.where(diag == diag.max())[0]  \n",
    "print('Number of clubs: {0}'.format(diag.max()))\n",
    "print('People with the most number of memberships:')\n",
    "for i in indices:\n",
    "    print('- {0}'.format(people_nodes[i]))\n",
    "\n",
    "# Set the diagonal to zero and convert it to a coordinate matrix format\n",
    "user_matrix.setdiag(0)\n",
    "users_coo = user_matrix.tocoo()\n",
    "\n",
    "# Find pairs of users who shared membership in the most number of clubs\n",
    "indices = np.where(users_coo.data == users_coo.data.max())[0]\n",
    "print('People with most number of shared memberships:')\n",
    "for idx in indices:\n",
    "    print('- {0}, {1}'.format(people_nodes[users_coo.row[idx]], people_nodes[users_coo.col[idx]]))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas\n",
    "### Make nodelist\n",
    "You're now going to practice converting graphs to pandas representation. If you have taken any of DataCamp's pandas courses, you will know that there is a DataFrame.to_csv('filename.csv') method that lets you save it as a CSV file, which is a human-readable version. The main concept we hope you take away from here is the process of converting a graph to a list of records.\n",
    "\n",
    "Start by re-familiarizing yourself with the graph data structure by calling G.nodes(data=True)[0] in the IPython Shell to examine one node in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store each edge as a record: nodelist\n",
    "nodelist = []\n",
    "for n, d in G_people.nodes(data=True):\n",
    "    # nodeinfo stores one \"record\" of data as a dict\n",
    "    nodeinfo = {'person': n} \n",
    "    \n",
    "    # Update the nodeinfo dictionary \n",
    "    nodeinfo.update(d)\n",
    "    \n",
    "    # Append the nodeinfo to the node list\n",
    "    nodelist.append(nodeinfo)\n",
    "    \n",
    "\n",
    "# Create a pandas DataFrame of the nodelist: node_df\n",
    "node_df = pd.DataFrame(nodelist)\n",
    "print(node_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make edgelist\n",
    "Now, you're going to apply the same ideas to making an edge list. Go forth and give it a shot!\n",
    "\n",
    "As with the previous exercise, run G.edges(data=True)[0] in the IPython Shell to get a feel for the edge list data structure before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store each edge as a record: edgelist\n",
    "edgelist = []\n",
    "for n1, n2, d in G_people.edges(data=True):\n",
    "    # Initialize a dictionary that shows edge information: edgeinfo\n",
    "    edgeinfo = {'node1':n1, 'node2':n2}\n",
    "    \n",
    "    # Update the edgeinfo data with the edge metadata\n",
    "    edgeinfo.update(d)\n",
    "    \n",
    "    # Append the edgeinfo to the edgelist\n",
    "    edgelist.append(edgeinfo)\n",
    "    \n",
    "# Create a pandas DataFrame of the edgelist: edge_df\n",
    "edge_df = pd.DataFrame(edgelist)\n",
    "print(edge_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of graphs\n",
    "In this set of exercises, you'll use a college messaging dataset to learn how to filter graphs for time series analysis. In this dataset, nodes are students, and edges denote messages being sent from one student to another. The graph as it stands right now captures all communications at all time points.\n",
    "\n",
    "Let's start by analyzing the graphs in which only the edges change over time.\n",
    "\n",
    "The dataset has been loaded into a DataFrame called data. Feel free to explore it in the IPython Shell. Specifically, check out the output of data['sender'] and data['recipient']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx \n",
    "\n",
    "months = range(4, 11)\n",
    "\n",
    "# Initialize an empty list: Gs\n",
    "Gs = [] \n",
    "for month in months:\n",
    "    # Instantiate a new undirected graph: G\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add in all nodes that have ever shown up to the graph\n",
    "    G.add_nodes_from(data['sender'])\n",
    "    G.add_nodes_from(data['recipient'])\n",
    "    \n",
    "    # Filter the DataFrame so that there's only the given month\n",
    "    df_filtered = data[data['month'] == month]\n",
    "    \n",
    "    # Add edges from filtered DataFrame\n",
    "    G.add_edges_from(zip(df_filtered['sender'], df_filtered['recipient']))\n",
    "    \n",
    "    # Append G to the list of graphs\n",
    "    Gs.append(G)\n",
    "    \n",
    "print(len(Gs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph differences over time\n",
    "Now, you'll compute the graph differences over time! To look at the simplest case, here you'll use a window of (month, month + 1), and then keep track of the edges gained or lost over time. This exercise is preparation for the next exercise, in which you will visualize the changes over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx  \n",
    "# Instantiate a list of graphs that show edges added: added\n",
    "added = []\n",
    "# Instantiate a list of graphs that show edges removed: removed\n",
    "removed = []\n",
    "# Here's the fractional change over time\n",
    "fractional_changes = []\n",
    "window = 1  \n",
    "i = 0      \n",
    "\n",
    "for i in range(len(Gs) - window):\n",
    "    g1 = Gs[i]\n",
    "    g2 = Gs[i + window]\n",
    "        \n",
    "    # Compute graph difference here\n",
    "    added.append(nx.difference(g2, g1))   \n",
    "    removed.append(nx.difference(g1, g2))\n",
    "    \n",
    "    # Compute change in graph size over time\n",
    "    fractional_changes.append((len(g2.edges()) - len(g1.edges())) / len(g1.edges()))\n",
    "    \n",
    "# Print the fractional change\n",
    "print(fractional_changes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot number of edge changes over time\n",
    "You're now going to make some plots! All of the lists that you've created before have been loaded for you in this exercise too. Do not worry about some of the fancy matplotlib code that shows up below: there are comments to help you understand what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "# Plot the number of edges added over time\n",
    "edges_added = [len(g.edges()) for g in added]\n",
    "plot1 = ax1.plot(edges_added, label='added', color='orange')\n",
    "\n",
    "# Plot the number of edges removed over time\n",
    "edges_removed = [len(g.edges()) for g in removed]\n",
    "plot2 = ax1.plot(edges_removed, label='removed', color='purple')\n",
    "\n",
    "# Set yscale to logarithmic scale\n",
    "ax1.set_yscale('log')  \n",
    "ax1.legend()\n",
    "\n",
    "# 2nd axes shares x-axis with 1st axes object\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# Plot the fractional changes over time\n",
    "plot3 = ax2.plot(fractional_changes, label='fractional change', color='green')\n",
    "\n",
    "# Here, we create a single legend for both plots\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax2.legend(lines1 + lines2, labels1 + labels2, loc=0)\n",
    "plt.axhline(0, color='green', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of edges over time\n",
    "You're now going to get some practice plotting other evolving graph statistics. We'll start with a simpler exercise to kick things off. First off, plot the number of edges over time.\n",
    "\n",
    "To do this, you'll create a list of the number of edges per month. The index of this list will correspond to the months elapsed since the first month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "# Create a list of the number of edges per month\n",
    "edge_sizes = [len(g.edges()) for g in Gs]\n",
    "\n",
    "# Plot edge sizes over time\n",
    "plt.plot(edge_sizes)\n",
    "plt.xlabel('Time elapsed from first month (in months).') \n",
    "plt.ylabel('Number of edges')                           \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degree centrality over time\n",
    "Now, you're going to plot the degree centrality distribution over time. Remember that the ECDF function will be provided, so you won't have to implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a list of degree centrality scores month-by-month\n",
    "cents = []\n",
    "for G in Gs:\n",
    "    cent = nx.degree_centrality(G)\n",
    "    cents.append(cent)\n",
    "\n",
    "\n",
    "# Plot ECDFs over time\n",
    "fig = plt.figure()\n",
    "for i in range(len(cents)):\n",
    "    x, y = ECDF(cents[i].values()) \n",
    "    plt.plot(x, y, label='Month {0}'.format(i+1)) \n",
    "plt.legend()   \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find nodes with top degree centralities\n",
    "In this exercise, you'll take a deeper dive to see whether there's anything interesting about the most connected students in the network. First off, you'll find the cluster of students that have the highest degree centralities. This result will be saved for the next plotting exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 5 unique degree centrality scores: top_dcs\n",
    "top_dcs = sorted(set(nx.degree_centrality(G).values()), reverse=True)[0:5]\n",
    "\n",
    "# Create list of nodes that have the top 5 highest overall degree centralities\n",
    "top_connected = []\n",
    "for n, dc in nx.degree_centrality(G).items():\n",
    "    if dc in top_dcs:\n",
    "        top_connected.append(n)\n",
    "        \n",
    "# Print the number of nodes that share the top 5 degree centrality scores\n",
    "print(len(top_connected))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing connectivity\n",
    "Here, you're going to visualize how the connectivity of the top connected nodes changes over time. The list of top connected values, top_connected, from the previous exercise has been loaded.\n",
    "\n",
    "Remember the defaultdict you used in Chapter 1? You'll use another defaultdict in this exercise! As Eric mentioned in the video, a defaultdict is preferred here as a regular Python dictionary would throw a KeyError if you try to get an item with a key that is not currently in the dictionary.\n",
    "\n",
    "Notice how there seems to be two distinct clusters of nodes that behave slightly differently: nodes 32, 9 and 41 are one group, and the nodes 400, 105, and 103 are the other. There may be something interesting to investigate going forward!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# Create a defaultdict in which the keys are nodes and the values are a list of connectivity scores over time\n",
    "connectivity = defaultdict(list)\n",
    "for n in top_connected:\n",
    "    for g in Gs:\n",
    "        connectivity[n].append(len(g.neighbors(n)))\n",
    "\n",
    "# Plot the connectivity for each node\n",
    "fig = plt.figure() \n",
    "for n, conn in connectivity.items(): \n",
    "    plt.plot(conn, label=n) \n",
    "plt.legend()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final projects: student posts to forums\n",
    "### Create a graph from the pandas DataFrame\n",
    "Let's start by creating a graph from a pandas DataFrame. In this exercise, you'll create a new bipartite graph by looping over the edgelist (which is a DataFrame object).\n",
    "\n",
    "For simplicity's sake, in this graph construction procedure, any edge between a student and a forum node will be the 'last' edge (in time) that a student posted to a forum over the entire time span of the dataset, though there are ways to get around this.\n",
    "\n",
    "Additionally, to shorten the runtime of the exercise, we have provided a sub-sampled version of the edge list as data. Explore it in the IPython Shell to familiarize yourself with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# Instantiate a new Graph: G\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes from each of the partitions\n",
    "G.add_nodes_from(data['student'], bipartite='student')\n",
    "G.add_nodes_from(data['forum'], bipartite='forum')\n",
    "\n",
    "# Add in each edge along with the date the edge was created\n",
    "for r, d in data.iterrows():\n",
    "    G.add_edge(d['student'], d['forum'], date=d['date']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the degree centrality distribution of the students projection\n",
    "In this exercise, you will visualize the degree centrality distribution of the students projection. This is a recap of two previous concepts you've learned: degree centralities, and projections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "# Get the student partition's nodes: student_nodes\n",
    "student_nodes = [n for n, d in G.nodes(data=True) if d['bipartite'] == 'student']\n",
    "\n",
    "# Create the students nodes projection as a graph: G_students\n",
    "G_students = nx.bipartite.projected_graph(G, nodes=student_nodes)\n",
    "\n",
    "# Calculate the degree centrality using nx.degree_centrality: dcs\n",
    "dcs = nx.degree_centrality(G_students)\n",
    "\n",
    "# Plot the histogram of degree centrality values\n",
    "plt.hist(list(dcs.values()))\n",
    "plt.yscale('log')  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the degree centrality distribution of the forums projection\n",
    "This exercise is also to reinforce the concepts of degree centrality and projections. This time round, you'll plot the degree centrality distribution for the 'forum' projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import matplotlib.pyplot as plt \n",
    "import networkx as nx\n",
    "\n",
    "# Get the forums partition's nodes: forum_nodes\n",
    "forum_nodes = [n for n, d in G.nodes(data=True) if d['bipartite'] == 'forum']\n",
    "\n",
    "# Create the forum nodes projection as a graph: G_forum\n",
    "G_forum = nx.bipartite.projected_graph(G, nodes=forum_nodes)\n",
    "\n",
    "# Calculate the degree centrality using nx.degree_centrality: dcs\n",
    "dcs = nx.degree_centrality(G_forum)\n",
    "\n",
    "# Plot the histogram of degree centrality values\n",
    "plt.hist(list(dcs.values()))\n",
    "plt.yscale('log') \n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time filter on edges\n",
    "You're now going to practice filtering the graph using a conditional as applied to the edges. This will help you gain practice and become comfortable with list comprehensions that contain conditionals.\n",
    "\n",
    "To help you with the exercises, remember that you can import datetime objects from the datetime module. On the graph, the metadata has a date key that is paired with a datetime object as a value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from datetime import datetime\n",
    "\n",
    "# Instantiate a new graph: G_sub\n",
    "G_sub = nx.Graph()\n",
    "\n",
    "# Add nodes from the original graph\n",
    "G_sub.add_nodes_from(G.nodes(data=True))\n",
    "\n",
    "# Add edges using a list comprehension with one conditional on the edge dates, that the date of the edge is earlier than 2004-05-16.\n",
    "G_sub.add_edges_from([(u, v, d) for u, v, d in G.edges(data=True) if d['date'] < datetime(2004, 5, 16)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize filtered graph using nxviz\n",
    "Here, you'll visualize the filtered graph using a CircosPlot. The CircosPlot is a natural choice for this visualization, as you can use node grouping and coloring to visualize the partitions, while the circular layout preserves the aesthetics of the visualization.\n",
    "\n",
    "We see that the most connected users overall aren't posting to the most popular forums early on. This trend may change as we go forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from nxviz import CircosPlot\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute degree centrality scores of each node\n",
    "dcs = nx.bipartite.degree_centrality(G, nodes=forum_nodes)\n",
    "for n, d in G_sub.nodes(data=True):\n",
    "    G_sub.node[n]['dc'] = dcs[n]\n",
    "\n",
    "# Create the CircosPlot object: c\n",
    "c = CircosPlot(G_sub, node_color='bipartite', node_grouping='bipartite', node_order='dc')\n",
    "\n",
    "# Draw c to screen\n",
    "c.draw()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot number of posts being made over time\n",
    "Let's recap how you can plot evolving graph statistics from the graph data. First off, you will use the graph data to quantify the number of edges that show up within a chunking time window of td days, which is 2 days in the exercise below.\n",
    "\n",
    "We can see the initial spike in the number of posts, but it eventually flattens out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from datetime import timedelta  \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define current day and timedelta of 2 days\n",
    "curr_day = dayone\n",
    "td = timedelta(days=2)\n",
    "\n",
    "# Initialize an empty list of posts by day\n",
    "n_posts = []\n",
    "while curr_day < lastday:\n",
    "    if curr_day.day == 1:\n",
    "        print(curr_day) \n",
    "    # Filter edges such that they are within the sliding time window: edges\n",
    "    edges = [(u, v, d) for u, v, d in G.edges(data=True) if d['date'] >= curr_day and d['date'] < curr_day + td]\n",
    "    \n",
    "    # Append number of edges to the n_posts list\n",
    "    n_posts.append(len(edges))\n",
    "    \n",
    "    # Increment the curr_day by the time delta\n",
    "    curr_day += td\n",
    "    \n",
    "# Create the plot\n",
    "plt.plot(n_posts)  \n",
    "plt.xlabel('Days elapsed')\n",
    "plt.ylabel('Number of posts')\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the mean degree centrality day-by-day on the students partition\n",
    "Here, you're going to see if the mean degree centrality over all nodes is correlated with the number of edges that are plotted over time. There might not necessarily be a strong correlation, and you'll take a look to see if that's the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize a new list: mean_dcs\n",
    "mean_dcs = []\n",
    "curr_day = dayone\n",
    "td = timedelta(days=2)\n",
    "\n",
    "while curr_day < lastday:\n",
    "    if curr_day.day == 1:\n",
    "        print(curr_day) \n",
    "    # Instantiate a new graph containing a subset of edges: G_sub\n",
    "    G_sub = nx.Graph()\n",
    "    # Add nodes from G\n",
    "    G_sub.add_nodes_from(G.nodes(data=True))\n",
    "    # Add in edges that fulfill the criteria\n",
    "    G_sub.add_edges_from([(u, v, d) for u, v, d in G.edges(data=True) if d['date'] >= curr_day and d['date'] < curr_day + td])\n",
    "    \n",
    "    # Get the students projection\n",
    "    G_student_sub = nx.bipartite.projected_graph(G_sub, nodes=student_nodes)\n",
    "    # Compute the degree centrality of the students projection\n",
    "    dc = nx.degree_centrality(G_student_sub)\n",
    "    # Append mean degree centrality to the list mean_dcs\n",
    "    mean_dcs.append(np.mean(list(dc.values())))\n",
    "    # Increment the time\n",
    "    curr_day += td\n",
    "    \n",
    "plt.plot(mean_dcs)\n",
    "plt.xlabel('Time elapsed')\n",
    "plt.ylabel('Degree centrality.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the most popular forums day-by-day: I\n",
    "We're going to see how many forums took the title of \"the most popular forum\" on any given time window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from datetime import timedelta\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Instantiate a list to hold the list of most popular forums by day: most_popular_forums\n",
    "most_popular_forums = []\n",
    "# Instantiate a list to hold the degree centrality scores of the most popular forums: highest_dcs\n",
    "highest_dcs = []\n",
    "curr_day = dayone  \n",
    "td = timedelta(days=1)  \n",
    "\n",
    "while curr_day < lastday:  \n",
    "    if curr_day.day == 1: \n",
    "        print(curr_day) \n",
    "    # Instantiate new graph: G_sub\n",
    "    G_sub = nx.Graph()\n",
    "    \n",
    "    # Add in nodes from original graph G\n",
    "    G_sub.add_nodes_from(G.nodes(data=True))\n",
    "    \n",
    "    # Add in edges from the original graph G that fulfill the criteria\n",
    "    G_sub.add_edges_from([(u, v, d) for u, v, d in G.edges(data=True) if d['date'] >= curr_day and d['date'] < curr_day + td])\n",
    "    \n",
    "    # CODE CONTINUES ON NEXT EXERCISE\n",
    "    curr_day += td"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the most popular forums day-by-day: II\n",
    "Now, you're going to finish that exercise - that is, you'll find out how many forums had the most popular forum score on a per-day basis!\n",
    "\n",
    "One of the things you will be doing here is a \"dictionary comprehension\" to filter a dictionary. It is very similar to a list comprehension to filter a list, except the syntax looks like: {key: val for key, val in dict.items() if ...}. Keep that in mind!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from datetime import timedelta\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "most_popular_forums = []\n",
    "highest_dcs = []\n",
    "curr_day = dayone \n",
    "td = timedelta(days=1)  \n",
    "\n",
    "while curr_day < lastday:  \n",
    "    if curr_day.day == 1:  \n",
    "        print(curr_day)  \n",
    "    G_sub = nx.Graph()\n",
    "    G_sub.add_nodes_from(G.nodes(data=True))   \n",
    "    G_sub.add_edges_from([(u, v, d) for u, v, d in G.edges(data=True) if d['date'] >= curr_day and d['date'] < curr_day + td])\n",
    "    \n",
    "    # Get the degree centrality \n",
    "    dc = nx.bipartite.degree_centrality(G_sub, forum_nodes)\n",
    "    # Filter the dictionary such that there's only forum degree centralities\n",
    "    forum_dcs = {n:dc for n, dc in dc.items() if n in forum_nodes}\n",
    "    # Identify the most popular forum(s) \n",
    "    most_popular_forum = [n for n, dc in dc.items() if dc == max(forum_dcs.values()) and dc != 0] \n",
    "    most_popular_forums.append(most_popular_forum) \n",
    "    # Store the highest dc values in highest_dcs\n",
    "    highest_dcs.append(max(forum_dcs.values()))\n",
    "    \n",
    "    curr_day += td  \n",
    "    \n",
    "plt.figure(1) \n",
    "plt.plot([len(forums) for forums in most_popular_forums], color='blue', label='Forums')\n",
    "plt.ylabel('Number of Most Popular Forums')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(2)\n",
    "plt.plot(highest_dcs, color='orange', label='DC Score')\n",
    "plt.ylabel('Top Degree Centrality Score')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
