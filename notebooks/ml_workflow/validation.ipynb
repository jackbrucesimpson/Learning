{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "- Always use Cross-Validation\n",
    "- \n",
    "\n",
    "Create a hold-out validation dataset for use later.\n",
    "\n",
    "How to Train a Final Machine Learning Model\n",
    "\n",
    "by Jason Brownlee on March 17, 2017 in Machine Learning Process\n",
    "The machine learning model that we use to make predictions on new data is called the final model.\n",
    "\n",
    "There can be confusion in applied machine learning about how to train a final model.\n",
    "\n",
    "This error is seen with beginners to the field who ask questions such as:\n",
    "\n",
    "How do I predict with cross validation?\n",
    "Which model do I choose from cross-validation?\n",
    "Do I use the model after preparing it on the training dataset?\n",
    "This post will clear up the confusion.\n",
    "\n",
    "In this post, you will discover how to finalize your machine learning model in order to make predictions on new data.\n",
    "\n",
    "Let’s get started.\n",
    "\n",
    "How to Train a Final Machine Learning Model\n",
    "How to Train a Final Machine Learning Model\n",
    "Photo by Camera Eye Photography, some rights reserved.\n",
    "What is a Final Model?\n",
    "\n",
    "A final machine learning model is a model that you use to make predictions on new data.\n",
    "\n",
    "That is, given new examples of input data, you want to use the model to predict the expected output. This may be a classification (assign a label) or a regression (a real value).\n",
    "\n",
    "For example, whether the photo is a picture of a dog or a cat, or the estimated number of sales for tomorrow.\n",
    "\n",
    "The goal of your machine learning project is to arrive at a final model that performs the best, where “best” is defined by:\n",
    "\n",
    "Data: the historical data that you have available.\n",
    "Time: the time you have to spend on the project.\n",
    "Procedure: the data preparation steps, algorithm or algorithms, and the chosen algorithm configurations.\n",
    "In your project, you gather the data, spend the time you have, and discover the data preparation procedures, algorithm to use, and how to configure it.\n",
    "\n",
    "The final model is the pinnacle of this process, the end you seek in order to start actually making predictions.\n",
    "\n",
    "The Purpose of Train/Test Sets\n",
    "\n",
    "Why do we use train and test sets?\n",
    "\n",
    "Creating a train and test split of your dataset is one method to quickly evaluate the performance of an algorithm on your problem.\n",
    "\n",
    "The training dataset is used to prepare a model, to train it.\n",
    "\n",
    "We pretend the test dataset is new data where the output values are withheld from the algorithm. We gather predictions from the trained model on the inputs from the test dataset and compare them to the withheld output values of the test set.\n",
    "\n",
    "Comparing the predictions and withheld outputs on the test dataset allows us to compute a performance measure for the model on the test dataset. This is an estimate of the skill of the algorithm trained on the problem when making predictions on unseen data.\n",
    "\n",
    "Let’s unpack this further\n",
    "\n",
    "When we evaluate an algorithm, we are in fact evaluating all steps in the procedure, including how the training data was prepared (e.g. scaling), the choice of algorithm (e.g. kNN), and how the chosen algorithm was configured (e.g. k=3).\n",
    "\n",
    "The performance measure calculated on the predictions is an estimate of the skill of the whole procedure.\n",
    "\n",
    "We generalize the performance measure from:\n",
    "\n",
    "“the skill of the procedure on the test set“\n",
    "to\n",
    "\n",
    "“the skill of the procedure on unseen data“.\n",
    "This is quite a leap and requires that:\n",
    "\n",
    "The procedure is sufficiently robust that the estimate of skill is close to what we actually expect on unseen data.\n",
    "The choice of performance measure accurately captures what we are interested in measuring in predictions on unseen data.\n",
    "The choice of data preparation is well understood and repeatable on new data, and reversible if predictions need to be returned to their original scale or related to the original input values.\n",
    "The choice of algorithm makes sense for its intended use and operational environment (e.g. complexity or chosen programming language).\n",
    "A lot rides on the estimated skill of the whole procedure on the test set.\n",
    "\n",
    "In fact, using the train/test method of estimating the skill of the procedure on unseen data often has a high variance (unless we have a heck of a lot of data to split). This means that when it is repeated, it gives different results, often very different results.\n",
    "\n",
    "The outcome is that we may be quite uncertain about how well the procedure actually performs on unseen data and how one procedure compares to another.\n",
    "\n",
    "Often, time permitting, we prefer to use k-fold cross-validation instead.\n",
    "\n",
    "The Purpose of k-fold Cross Validation\n",
    "\n",
    "Why do we use k-fold cross validation?\n",
    "\n",
    "Cross-validation is another method to estimate the skill of a method on unseen data. Like using a train-test split.\n",
    "\n",
    "Cross-validation systematically creates and evaluates multiple models on multiple subsets of the dataset.\n",
    "\n",
    "This, in turn, provides a population of performance measures.\n",
    "\n",
    "We can calculate the mean of these measures to get an idea of how well the procedure performs on average.\n",
    "We can calculate the standard deviation of these measures to get an idea of how much the skill of the procedure is expected to vary in practice.\n",
    "\n",
    "\n",
    "### Classification accuracy is the ratio of correct predictions to total predictions made.\n",
    "\n",
    "A confusion matrix is a summary of prediction results on a classification problem.\n",
    "\n",
    "The number of correct and incorrect predictions are summarized with count values and broken down by each class. This is the key to the confusion matrix.\n",
    "\n",
    "Regression problems are those where a real value is predicted.\n",
    "\n",
    "An easy metric to consider is the error in the predicted values as compared to the expected values.\n",
    "\n",
    "The Mean Absolute Error or MAE for short is a good first error metric to use.\n",
    "\n",
    "It is calculated as the average of the absolute error values, where “absolute” means “made positive” so that they can be added together.\n",
    "\n",
    "MAE = sum( abs(predicted_i - actual_i) ) / total predictions\n",
    "\n",
    "Root Mean Squared Error\n",
    "\n",
    "Another popular way to calculate the error in a set of regression predictions is to use the Root Mean Squared Error.\n",
    "\n",
    "Shortened as RMSE, the metric is sometimes called Mean Squared Error or MSE, dropping the Root part from the calculation and the name.\n",
    "\n",
    "RMSE is calculated as the square root of the mean of the squared differences between actual outcomes and predictions.\n",
    "\n",
    "Squaring each error forces the values to be positive, and the square root of the mean squared error returns the error metric back to the original units for comparison.\n",
    "\n",
    "Running the example, we see the results below. The result is slightly higher at 0.0089.\n",
    "\n",
    "RMSE values are always slightly higher than MSE values, which becomes more pronounced as the prediction errors increase. This is a benefit of using RMSE over MSE in that it penalizes larger errors with worse scores.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training dataset is used by the machine learning algorithm to train the model. The test dataset is held back and is used to evaluate the performance of the model.\n",
    "\n",
    "The k-fold cross validation method (also called just cross validation) is a resampling method that provides a more accurate estimate of algorithm performance.\n",
    "\n",
    "It does this by first splitting the data into k groups. The algorithm is then trained and evaluated k times and the performance summarized by taking the mean performance score. Each group of data is called a fold, hence the name k-fold cross-validation.\n",
    "\n",
    "It works by first training the algorithm on the k-1 groups of the data and evaluating it on the kth hold-out group as the test set. This is repeated so that each of the k groups is given an opportunity to be held out and used as the test set.\n",
    "\n",
    "As such, the value of k should be divisible by the number of rows in your training dataset, to ensure each of the k groups has the same number of rows.\n",
    "\n",
    "You should choose a value for k that splits the data into groups with enough rows that each group is still representative of the original dataset. A good default to use is k=3 for a small dataset or k=10 for a larger dataset. A quick way to check if the fold sizes are representative is to calculate summary statistics such as mean and standard deviation and see how much the values differ from the same statistics on the whole dataset.\n",
    "\n",
    "We can reuse what we learned in the previous section in creating a train and test split here in implementing k-fold cross validation.\n",
    "\n",
    "Instead of two groups, we must return k-folds or k groups of data.\n",
    "\n",
    "Below is a function named cross_validation_split() that implements the cross validation split of data.\n",
    "\n",
    "As before, we create a copy of the dataset from which to draw randomly chosen rows.\n",
    "\n",
    "We calculate the size of each fold as the size of the dataset divided by the number of folds required.\n",
    "\n",
    "fold size = total rows / total folds\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\t•\tSampling: There may be far more selected data available than you need to work with. More data can result in much longer running times for algorithms and larger computational and memory requirements. You can take a smaller representative sample of the selected data that may be much faster for exploring and prototyping solutions before considering the whole dataset.\n",
    "\t•\tScaling: The preprocessed data may contain attributes with a mixtures of scales for various quantities such as dollars, kilograms and sales volume. Many machine learning methods like data attributes to have the same scale such as between 0 and 1 for the smallest and largest value for a given feature. Consider any feature scaling you may need to perform.\n",
    "\t•\tDecomposition: There may be features that represent a complex concept that may be more useful to a machine learning method when split into the constituent parts. An example is a date that may have day and time components that in turn could be split out further. Perhaps only the hour of day is relevant to the problem being solved. consider what feature decompositions you can perform.\n",
    "\t•\tAggregation: There may be features that can be aggregated into a single feature that would be more meaningful to the problem you are trying to solve. For example, there may be a data instances for each time a customer logged into a system that could be aggregated into a count for the number of logins allowing the additional instances to be discarded. Consider what type of feature aggregations could perform.\n",
    "\t•\t\n",
    "\n",
    "\n",
    "Cross Validation\n",
    "A more sophisticated approach than using a test and train dataset is to use the entire transformed dataset to train and test a given algorithm. A method you could use in your test harness that does this is called cross validation.\n",
    "It first involves separating the dataset into a number of equally sized groups of instances (called folds). The model is then trained on all folds exception one that was left out and the prepared model is tested on that left out fold. The process is repeated so that each fold get’s an opportunity at being left out and acting as the test dataset. Finally, the performance measures are averaged across all folds to estimate the capability of the algorithm on the problem.\n",
    "For example, a 3-fold cross validation would involve training and testing a model 3 times:\n",
    "\t•\t#1: Train on folds 1+2, test on fold 3\n",
    "\t•\t#2: Train on folds 1+3, test on fold 2\n",
    "\t•\t#3: Train on folds 2+3, test on fold 1\n",
    "The number of folds can vary based on the size of your dataset, but common numbers are 3, 5, 7 and 10 folds. The goal is to have a good balance between the size and representation of data in your train and test sets.\n",
    "When you’re just getting started, stick with a simple split of train and test data (such as 66%/34%) and move onto cross validation once you have more confidence.\n",
    "\n",
    "\n",
    "\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.shape)\n",
    "View dimensions\n",
    "\n",
    "Peak at data\n",
    "\n",
    "Stats summary\n",
    "\n",
    "We will use 10-fold cross validation to estimate accuracy.\n",
    "This will split our dataset into 10 parts, train on 9 and test on 1 and repeat for all combinations of train-test splits.\n",
    "\n",
    "\n",
    "seed = 7\n",
    "scoring = 'accuracy'\n",
    "\n",
    "We don’t know which algorithms would be good on this problem or what configurations to use. We get an idea from the plots that some of the classes are partially linearly separable in some dimensions, so we are expecting generally good results.\n",
    "Let’s evaluate 6 different algorithms:\n",
    "\t•\tLogistic Regression (LR)\n",
    "\t•\tLinear Discriminant Analysis (LDA)\n",
    "\t•\tK-Nearest Neighbors (KNN).\n",
    "\t•\tClassification and Regression Trees (CART).\n",
    "\t•\tGaussian Naive Bayes (NB).\n",
    "\t•\tSupport Vector Machines (SVM).\n",
    "\n",
    "\n",
    "\n",
    "This is a good mixture of simple linear (LR and LDA), nonlinear (KNN, CART, NB and SVM) algorithms. We reset the random number seed before each run to ensure that the evaluation of each algorithm is performed using exactly the same data splits. It ensures the results are directly comparable.\n",
    "\n",
    "# Spot Check Algorithms\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC()))\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "\tkfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "\tcv_results = model_selection.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "\tresults.append(cv_results)\n",
    "\tnames.append(name)\n",
    "\tmsg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "\tprint(msg)\n",
    "\n",
    "\n",
    "# Compare Algorithms\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()\n",
    "\n",
    "The KNN algorithm was the most accurate model that we tested. Now we want to get an idea of the accuracy of the model on our validation set.\n",
    "This will give us an independent final check on the accuracy of the best model. It is valuable to keep a validation set just in case you made a slip during training, such as overfitting to the training set or a data leak. Both will result in an overly optimistic result.\n",
    "We can run the KNN model directly on the validation set and summarize the results as a final accuracy score, a confusion matrix and a classification report.\n",
    "\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, Y_train)\n",
    "predictions = knn.predict(X_validation)\n",
    "print(accuracy_score(Y_validation, predictions))\n",
    "print(confusion_matrix(Y_validation, predictions))\n",
    "print(classification_report(Y_validation, predictions))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Compare Machine Learning Algorithms Consistently\n",
    "The key to a fair comparison of machine learning algorithms is ensuring that each algorithm is evaluated in the same way on the same data.\n",
    "You can achieve this by forcing each algorithm to be evaluated on a consistent test harness.\n",
    "In the example below 6 different algorithms are compared:\n",
    "\t1.\tLogistic Regression\n",
    "\t2.\tLinear Discriminant Analysis\n",
    "\t3.\tK-Nearest Neighbors\n",
    "\t4.\tClassification and Regression Trees\n",
    "\t5.\tNaive Bayes\n",
    "\t6.\tSupport Vector Machines\n",
    "The problem is a standard binary classification dataset from the UCI machine learning repository called the Pima Indians onset of diabetes problem. The problem has two classes and eight numeric input variables of varying scales.\n",
    "The 10-fold cross validation procedure is used to evaluate each algorithm, importantly configured with the same random seed to ensure that the same splits to the training data are performed and that each algorithms is evaluated in precisely the same way.\n",
    "Each algorithm is given a short name, useful for summarizing results afterward.\n",
    "\n",
    "\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "# load dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "# prepare configuration for cross validation test harness\n",
    "seed = 7\n",
    "# prepare models\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC()))\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "for name, model in models:\n",
    "\tkfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "\tcv_results = model_selection.cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "\tresults.append(cv_results)\n",
    "\tnames.append(name)\n",
    "\tmsg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "\tprint(msg)\n",
    "# boxplot algorithm comparison\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "Running the example provides a list of each algorithm short name, the mean accuracy and the standard deviation accuracy.\n",
    "\n",
    "LDA: 0.773462 (0.051592)\n",
    "KNN: 0.726555 (0.061821)\n",
    "CART: 0.695232 (0.062517)\n",
    "NB: 0.755178 (0.042766)\n",
    "SVM: 0.651025 (0.072141)\n",
    "\n",
    "\n",
    "\n",
    "The example also provides a box and whisker plot showing the spread of the accuracy scores across each cross validation fold for each algorithm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transform Data\n",
    "The final step is to transform the process data. The specific algorithm you are working with and the knowledge of the problem domain will influence this step and you will very likely have to revisit different transformations of your preprocessed data as you work on your problem.\n",
    "Three common data transformations are scaling, attribute decompositions and attribute aggregations. This step is also referred to as feature engineering.\n",
    "\t•\tScaling: The preprocessed data may contain attributes with a mixtures of scales for various quantities such as dollars, kilograms and sales volume. Many machine learning methods like data attributes to have the same scale such as between 0 and 1 for the smallest and largest value for a given feature. Consider any feature scaling you may need to perform.\n",
    "\t•\tDecomposition: There may be features that represent a complex concept that may be more useful to a machine learning method when split into the constituent parts. An example is a date that may have day and time components that in turn could be split out further. Perhaps only the hour of day is relevant to the problem being solved. consider what feature decompositions you can perform.\n",
    "\t•\tAggregation: There may be features that can be aggregated into a single feature that would be more meaningful to the problem you are trying to solve. For example, there may be a data instances for each time a customer logged into a system that could be aggregated into a count for the number of logins allowing the additional instances to be discarded. Consider what type of feature aggregations could perform.\n",
    "\t•\t\n",
    "\n",
    "\n",
    "Histograms to summarize the distribution of individual data attributes.\n",
    "Pairwise Histograms to plot attributes against each other and highlight relationships and outliers\n",
    "Dimensionality Reduction methods for creating lower dimensional plots and models of the data\n",
    "Clustering to expose natural groupings in the data\n",
    "    \n",
    "    Real data can have inconsistencies, missing values and various other forms of corruption. If it was scraped from a difficult data source, it may require tripping and cleaning up. Even clean data may require post-processing to make it uniform and consistent\n",
    "\n",
    "\n",
    "\n",
    "Cross Validation\n",
    "A more sophisticated approach than using a test and train dataset is to use the entire transformed dataset to train and test a given algorithm. A method you could use in your test harness that does this is called cross validation.\n",
    "It first involves separating the dataset into a number of equally sized groups of instances (called folds). The model is then trained on all folds exception one that was left out and the prepared model is tested on that left out fold. The process is repeated so that each fold get’s an opportunity at being left out and acting as the test dataset. Finally, the performance measures are averaged across all folds to estimate the capability of the algorithm on the problem.\n",
    "For example, a 3-fold cross validation would involve training and testing a model 3 times:\n",
    "\t•\t#1: Train on folds 1+2, test on fold 3\n",
    "\t•\t#2: Train on folds 1+3, test on fold 2\n",
    "\t•\t#3: Train on folds 2+3, test on fold 1\n",
    "The number of folds can vary based on the size of your dataset, but common numbers are 3, 5, 7 and 10 folds. The goal is to have a good balance between the size and representation of data in your train and test sets.\n",
    "When you’re just getting started, stick with a simple split of train and test data (such as 66%/34%) and move onto cross validation once you have more confidence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
