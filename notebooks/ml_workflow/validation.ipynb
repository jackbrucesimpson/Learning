{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation\n",
    "- Use to determine final ML model you'll use to make predictions on unseen data\n",
    "\n",
    "## Test Splits\n",
    "\n",
    "### Training/Testing dataset\n",
    "- Split data into training set to train model on, and an unseen testing set to test it on later\n",
    "- When you’re just getting started, stick with a simple split of train and test data (such as 66%/34%) and move onto cross validation once you have more confidence.\n",
    "- We will split the loaded dataset into two, 80% of which we will use to train our models and 20% that we will hold back as a validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split-out validation dataset\n",
    "array = dataset.values\n",
    "X = array[:,0:4]\n",
    "Y = array[:,4]\n",
    "validation_size = 0.20\n",
    "seed = 7\n",
    "X_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(X, Y, test_size=validation_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation: \n",
    "- Separating dataset into a number of equally sized groups of instances (called folds)\n",
    "- Model is then trained on all folds exception one that was left out and the prepared model is tested on that left out fold\n",
    "- The process is repeated so that each fold gets an opportunity at being left out and acting as the test dataset\n",
    "- Finally, the performance measures are averaged across all folds to estimate the capability of the algorithm on the problem.\n",
    "- e.g. 3-fold cross validation involves training and tests a model 3 times\n",
    "    - #1: Train on folds 1+2, test on fold 3\n",
    "    - #2: Train on folds 1+3, test on fold 2\n",
    "    - #3: Train on folds 2+3, test on fold 1\n",
    "- You should choose a value for k that splits the data into groups with enough rows that each group is still representative of the original dataset\n",
    "- A good default to use is k=3 for a small dataset or k=10 for a larger dataset\n",
    "- A quick way to check if the fold sizes are representative is to calculate summary statistics such as mean and standard deviation and see how much the values differ from the same statistics on the whole dataset.\n",
    "- The value of k should be divisible by the number of rows in your training dataset, to ensure each of the k groups has the same number of rows.\n",
    "- fold size = total rows / total folds\n",
    "- The number of folds can vary based on the size of your dataset, but common numbers are 3, 5, 7 and 10 folds. The goal is to have a good balance between the size and representation of data in your train and test sets.\n",
    "\n",
    "#### Example of k-folds validation\n",
    "- We will use 10-fold cross validation to estimate accuracy.\n",
    "- This will split our dataset into 10 parts, train on 9 and test on 1 and repeat for all combinations of train-test splits.\n",
    "- Evaluate 6 different algorithms:\n",
    "    - Logistic Regression (LR)\n",
    "    - Linear Discriminant Analysis (LDA)\n",
    "    - K-Nearest Neighbors (KNN).\n",
    "    - Classification and Regression Trees (CART).\n",
    "    - Gaussian Naive Bayes (NB).\n",
    "    - Support Vector Machines (SVM).\n",
    "- Mixture of simple linear (LR and LDA), nonlinear (KNN, CART, NB and SVM) algorithms.\n",
    "- We reset the random number seed before each run to ensure that the evaluation of each algorithm is performed using exactly the same data splits. It ensures the results are directly comparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "scoring = 'accuracy'\n",
    "\n",
    "# Spot Check Algorithms\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC()))\n",
    "\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "    cv_results = model_selection.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "\n",
    "# Compare Algorithms\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- KNN algorithm was the most accurate model that we tested. Now we want to get an idea of the accuracy of the model on our validation set.\n",
    "- This will give us an independent final check on the accuracy of the best model.\n",
    "- It is valuable to keep a validation set just in case you made a slip during training, such as overfitting to the training set or a data leak. Both will result in an overly optimistic result.\n",
    "- We can run the KNN model directly on the validation set and summarize the results as a final accuracy score, a confusion matrix and a classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, Y_train)\n",
    "predictions = knn.predict(X_validation)\n",
    "print(accuracy_score(Y_validation, predictions))\n",
    "print(confusion_matrix(Y_validation, predictions))\n",
    "print(classification_report(Y_validation, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another example\n",
    "- Retrieve via url and set column names\n",
    "- Running the example provides a list of each algorithm short name, the mean accuracy and the standard deviation accuracy.\n",
    "- The example also provides a box and whisker plot showing the spread of the accuracy scores across each cross validation fold for each algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# load dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "# prepare configuration for cross validation test harness\n",
    "seed = 7\n",
    "\n",
    "# prepare models\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC()))\n",
    "\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "for name, model in models:\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "    cv_results = model_selection.cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "\n",
    "# boxplot algorithm comparison\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "- Number of model Diagnostics: Learning curves, partial dependence plots, feature importances, ROC and other diagnostics are extremely useful to generate automatically.\n",
    "- Classification\n",
    "    - Classification accuracy is the ratio of correct predictions to total predictions made.\n",
    "    - Confusion matrix is a summary of prediction results on a classification problem: number of correct and incorrect predictions are summarized with count values and broken down by each class. \n",
    "- Regression\n",
    "    - Regression problems are those where a real value is predicted.\n",
    "    -  R^2\n",
    "    - An easy metric to consider is the error in the predicted values as compared to the expected values.\n",
    "    - The Mean Absolute Error (MAE)\n",
    "        - MAE for short is a good first error metric to use\n",
    "        - Calculated as the average of the absolute error values, where “absolute” means “made positive” so that they can be added together.\n",
    "        - MAE = sum( abs(predicted_i - actual_i) ) / total predictions\n",
    "    - Root Mean Squared Error (RMSE, sometimes called Mean Squared Error or MSE)\n",
    "        - Calculated as the square root of the mean of the squared differences between actual outcomes and predictions.\n",
    "        - Squaring each error forces the values to be positive, and the square root of the mean squared error returns the error metric back to the original units for comparison.\n",
    "        - RMSE values are always slightly higher than MSE values, which becomes more pronounced as the prediction errors increase. \n",
    "        - This is a benefit of using RMSE over MSE in that it penalizes larger errors with worse scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
