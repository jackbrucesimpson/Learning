{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration/Visualisation\n",
    "\n",
    "Do you suspect your data is “dirty” (has a few meaningless input patterns and/or noisy outputs or wrong class labels)? If yes, detect the outlier examples using the top ranking variables obtained in step 5 as representation; check and/or discard them.\n",
    "\n",
    "Rescale Data\n",
    "\n",
    "When your data is comprised of attributes with varying scales, many machine learning algorithms can benefit from rescaling the attributes to all have the same scale.\n",
    "\n",
    "Often this is referred to as normalization and attributes are often rescaled into the range between 0 and 1. This is useful for optimization algorithms in used in the core of machine learning algorithms like gradient descent. It is also useful for algorithms that weight inputs like regression and neural networks and algorithms that use distance measures like K-Nearest Neighbors.\n",
    "\n",
    "Automating tasks such as plotting all your variables against the target variable being predicted as well as computing summary statistics can save lots of time.\n",
    "\n",
    "Feature Transformations: There are many choices in how you can encode categorical variables, impute missing values, encode sequences and text, etc. Many of these feature transformations are canonical such that they can be reliably applied to many problems.\n",
    "\n",
    "Algorithm Selection & Hyper-parameter Tuning: There are a dizzying number of algorithms to choose from and related hyper-parameters that can be tuned. These tasks are very amenable to automation.\n",
    "\n",
    "Model Diagnostics: Learning curves, partial dependence plots, feature importances, ROC and other diagnostics are extremely useful to generate automatically.\n",
    "\n",
    "Missing data\n",
    "Incorrect datatypes\n",
    "\n",
    "Square and Cube\n",
    "Square root\n",
    "Standardize (e.g. 0 mean and unit variance)\n",
    "Normalize (e.g. rescale to 0-1)\n",
    "Descritize (e.g. convert a real to categorical)\n",
    "\n",
    "\n",
    "# Class Distribution (Classification Only)\n",
    "\n",
    "On classification problems you need to know how balanced the class values are.\n",
    "\n",
    "Highly imbalanced problems (a lot more observations for one class than another) are common and may need special handling in the data preparation stage of your project.\n",
    "\n",
    "You can quickly get an idea of the distribution of the class attribute in Pandas.\n",
    "\n",
    "```\n",
    "# Class Distribution\n",
    "import pandas\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "data = pandas.read_csv(url, names=names)\n",
    "class_counts = data.groupby('class').size()\n",
    "print(class_counts)\n",
    "```\n",
    "\n",
    "Skew, distribution, correlation\n",
    "\n",
    "On some columns, a value of zero does not make sense and indicates an invalid or missing value.\n",
    "\n",
    "dataset[[1,2,3,4,5]] = dataset[[1,2,3,4,5]].replace(0, numpy.NaN)\n",
    "\n",
    "Remove Rows With Missing Values\n",
    "dataset.dropna(inplace=True)\n",
    "\n",
    "Impute Missing Values\n",
    "\n",
    "A constant value that has meaning within the domain, such as 0, distinct from all other values.\n",
    "A value from another randomly selected record.\n",
    "A mean, median or mode value for the column.\n",
    "A value estimated by another predictive model.\n",
    "\n",
    "dataset.fillna(dataset.mean(), inplace=True)\n",
    "\n",
    "Sadly, the scikit-learn implementations of decision trees and k-Nearest Neighbors are not robust to missing values. Although it is being considered.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Rescale Data\n",
    "\n",
    "When your data is comprised of attributes with varying scales, many machine learning algorithms can benefit from rescaling the attributes to all have the same scale.\n",
    "\n",
    "Often this is referred to as normalization and attributes are often rescaled into the range between 0 and 1. This is useful for optimization algorithms in used in the core of machine learning algorithms like gradient descent. It is also useful for algorithms that weight inputs like regression and neural networks and algorithms that use distance measures like K-Nearest Neighbors.\n",
    "\n",
    "You can rescale your data using scikit-learn using the MinMaxScaler class.\n",
    "\n",
    "```python\n",
    "# Rescale data (between 0 and 1)\n",
    "import pandas\n",
    "import scipy\n",
    "import numpy\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "# separate array into input and output components\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "rescaledX = scaler.fit_transform(X)\n",
    "# summarize transformed data\n",
    "numpy.set_printoptions(precision=3)\n",
    "print(rescaledX[0:5,:])\n",
    "```\n",
    "\n",
    "2. Standardize Data\n",
    "\n",
    "Standardization is a useful technique to transform attributes with a Gaussian distribution and differing means and standard deviations to a standard Gaussian distribution with a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "It is most suitable for techniques that assume a Gaussian distribution in the input variables and work better with rescaled data, such as linear regression, logistic regression and linear discriminate analysis.\n",
    "\n",
    "You can standardize data using scikit-learn with the StandardScaler class.\n",
    "\n",
    "```python\n",
    "# Standardize data (0 mean, 1 stdev)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas\n",
    "import numpy\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "# separate array into input and output components\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "scaler = StandardScaler().fit(X)\n",
    "rescaledX = scaler.transform(X)\n",
    "# summarize transformed data\n",
    "numpy.set_printoptions(precision=3)\n",
    "print(rescaledX[0:5,:])\n",
    "```\n",
    "The values for each attribute now have a mean value of 0 and a standard deviation of 1.\n",
    "\n",
    "3. Normalize Data\n",
    "\n",
    "Normalizing in scikit-learn refers to rescaling each observation (row) to have a length of 1 (called a unit norm in linear algebra).\n",
    "\n",
    "This preprocessing can be useful for sparse datasets (lots of zeros) with attributes of varying scales when using algorithms that weight input values such as neural networks and algorithms that use distance measures such as K-Nearest Neighbors.\n",
    "\n",
    "You can normalize data in Python with scikit-learn using the Normalizer class.\n",
    "\n",
    "```python\n",
    "# Normalize data (length of 1)\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import pandas\n",
    "import numpy\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "# separate array into input and output components\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "scaler = Normalizer().fit(X)\n",
    "normalizedX = scaler.transform(X)\n",
    "# summarize transformed data\n",
    "numpy.set_printoptions(precision=3)\n",
    "print(normalizedX[0:5,:])\n",
    "```\n",
    "\n",
    "The rows are normalized to length 1.\n",
    "\n",
    "4. Binarize Data (Make Binary)\n",
    "```python\n",
    "# binarization\n",
    "from sklearn.preprocessing import Binarizer\n",
    "import pandas\n",
    "import numpy\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "# separate array into input and output components\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "binarizer = Binarizer(threshold=0.0).fit(X)\n",
    "binaryX = binarizer.transform(X)\n",
    "# summarize transformed data\n",
    "numpy.set_printoptions(precision=3)\n",
    "print(binaryX[0:5,:])\n",
    "```\n",
    "\n",
    "You can transform your data using a binary threshold. All values above the threshold are marked 1 and all equal to or below are marked as 0.\n",
    "\n",
    "This is called binarizing your data or threshold your data. It can be useful when you have probabilities that you want to make crisp values. It is also useful when feature engineering and you want to add new features that indicate something meaningful.\n",
    "\n",
    "You can create new binary attributes in Python using scikit-learn with the Binarizer class.\n",
    "\n",
    "https://machinelearningmastery.com/prepare-data-machine-learning-python-scikit-learn/\n",
    "\n",
    "\n",
    "Classification: when the function being learned is discrete.\n",
    "Regression: when the function being learned is continuous.\n",
    "Probability Estimation: when the output of the function is a probability.\n",
    "\n",
    "Supervised learning: (also called inductive learning) Training data includes desired outputs.  This is spam this is not, learning is supervised.\n",
    "Unsupervised learning: Training data does not include desired outputs. Example is clustering. It is hard to tell what is good learning and what is not.\n",
    "Semi-supervised learning: Training data includes a few desired outputs.\n",
    "Reinforcement learning: Rewards from a sequence of actions. AI types like it, it is the most ambitious type of learning.\n",
    "\n",
    "Representation. The classify you pick defines the representation the solution will take and the space of all learnable classifiers called the hypothesis space. Examples include instances, hyperplanes, decision trees, rule sets and neural networks.\n",
    "Evaluation. The evaluation function that you will use to judge a good classifier from a bad classifier. This may be the loss function used internally by the algorithm. Examples include accurate, squared error, likelihood and information gain.\n",
    "Optimization. The method to search for a good classifier. This is internally how the algorithm traverses the hypothesis space in the context of the evaluation function to a final classifier capable of making accurate predictions. Examples include combinatorial optimization and continuous optimization, and subsets thereof.\n",
    "2. It’s generalization that counts\n",
    "\n",
    "Bias is a learners tendency to learn the wrong thing. A linear learner has high bias because it is limited to separating classes using a hyperplane.\n",
    "Variance is the learners tendency to learn random things regardless of the real signal. Decision trees have high variance as they are highly influenced by the specifics in the training data.\n",
    "\n",
    "Intuition fails in high dimensions\n",
    "\n",
    "The second biggest problem in machine learning is the curse of dimensionality.\n",
    "\n",
    "\n",
    "\n",
    "Normalize Data\n",
    "\n",
    "Normalization can refer to different techniques depending on context.\n",
    "\n",
    "Here, we use normalization to refer to rescaling an input variable to the range between 0 and 1.\n",
    "\n",
    "Standardize Data\n",
    "\n",
    "Standardization is a rescaling technique that refers to centering the distribution of the data on the value 0 and the standard deviation to the value 1.\n",
    "\n",
    "Together, the mean and the standard deviation can be used to summarize a normal distribution, also called the Gaussian distribution or bell curve.\n",
    "\n",
    "When to Normalize and Standardize\n",
    "\n",
    "Standardization is a scaling technique that assumes your data conforms to a normal distribution.\n",
    "\n",
    "If a given data attribute is normal or close to normal, this is probably the scaling method to use.\n",
    "\n",
    "It is good practice to record the summary statistics used in the standardization process, so that you can apply them when standardizing data in the future that you may want to use with your model.\n",
    "\n",
    "Normalization is a scaling technique that does not assume any specific distribution.\n",
    "\n",
    "If your data is not normally distributed, consider normalizing it prior to applying your machine learning algorithm.\n",
    "\n",
    "It is good practice to record the minimum and maximum values for each column used in the normalization process, again, in case you need to normalize new data in the future to be used with your model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features\n",
    "\n",
    "Feature engineering is the key\n",
    "\n",
    "The factor that makes the biggest difference between machine learning projects that fail and succeed it is the features used.\n",
    "More data beats a cleverer algorithm\n",
    "\n",
    "Consider taking a closer took at the three most popular ensemble methods:\n",
    "\n",
    "Bagging: generate different samples of the training data, prepare a learner on each and combine the predictions using voting.\n",
    "Boosting: weight training instances by their difficulty during training to put special focus on those difficult to classify instances.\n",
    "Stacking: use a higher-level classifier to learn how to best combine the predictions of other classifiers.`\n",
    "\n",
    "Feature selection is another key part of the applied machine learning process, like model selection. You cannot fire and forget.\n",
    "For example, you must include feature selection within the inner-loop when you are using accuracy estimation methods such as cross-validation. This means that feature selection is performed on the prepared fold right before the model is trained. A mistake would be to perform feature selection first to prepare your data, then perform model selection and training on the selected features.\n",
    "Keeping irrelevant attributes in your dataset can result in overfitting\n",
    "\n",
    "- Are your features commensurate? If no, consider normalizing them.\n",
    "- Do you suspect interdependence of features? If yes, expand your feature set by constructing conjunctive features or products of features, as much as your computer resources allow you.\n",
    "- Do you need to assess features individually (e.g. to understand their influence on the system or because their number is so large that you need to do a first filtering)? If yes, use a variable ranking method; else, do it anyway to get baseline results.\n",
    "- Do you know what to try first? If no, use a linear predictor. Use a forward selection method with the “probe” method as a stopping criterion or use the 0-norm embedded method for comparison, following the ranking of step 5, construct a sequence of predictors of same nature using increasing subsets of features. Can you match or improve performance with a smaller subset? If yes, try a non-linear predictor with that subset.\n",
    "- \n",
    "\n",
    "### Feature Selection\n",
    "- include and exclude attributes present in the data without changing them\n",
    "- Fewer attributes is desirable because it reduces the complexity of the model\n",
    "- Feature selection methods can be used to identify and remove unneeded, irrelevant and redundant attributes from data that do not contribute to the accuracy of a predictive model or may in fact decrease the accuracy of the model\n",
    "- 3 classes of feature selection algorithms\n",
    "\n",
    "#### Filter Methods\n",
    "- apply a statistical measure to assign a scoring to each feature\n",
    "- features are ranked by the score and either selected to be kept or removed from the dataset\n",
    "- Chi squared test, information gain and correlation coefficient scores\n",
    "\n",
    "\n",
    "#### Wrapper Methods\n",
    "- consider the selection of a set of features as a search problem, where different combinations are prepared, evaluated and compared to other combinations. A predictive model us used to evaluate a combination of features and assign a score based on model accuracy\n",
    "- The search process may be methodical such as a best-first search, it may stochastic such as a random hill-climbing algorithm, or it may use heuristics, like forward and backward passes to add and remove features.\n",
    "- An example if a wrapper method is the recursive feature elimination algorithm.\n",
    "\n",
    "#### Embedded Methods\n",
    "- which features best contribute to the accuracy of the model while the model is being created. The most common type of embedded feature selection methods are regularization methods\n",
    "- Regularization methods are also called penalization methods that introduce additional constraints into the optimization of a predictive algorithm (such as a regression algorithm) that bias the model toward lower complexity (fewer coefficients).\n",
    "- Examples of regularization algorithms are the LASSO, Elastic Net and Ridge Regression.\n",
    "\n",
    "### Dimensionality reduction\n",
    "- creating new combinations of attributes\n",
    "- Principal Component Analysis, Singular Value Decomposition and Sammon’s Mapping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Data Types\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical\n",
    "A categorical variable has too many levels. This pulls down performance level of the model. For example, a cat. variable “zip code” would have numerous levels.\n",
    "A categorical variable has levels which rarely occur. Many of these levels have minimal chance of making a real impact on model fit. For example, a variable ‘disease’ might have some levels which would rarely occur.\n",
    "There is one level which always occurs i.e. for most of the observations in data set there is only one level. Variables with such levels fail to make a positive impact on model performance due to very low variation.\n",
    "If the categorical variable is masked, it becomes a laborious task to decipher its meaning. Such situations are commonly found in data science competitions.\n",
    "You can’t fit categorical variables into a regression equation in their raw form. They must be treated.\n",
    "Most of the algorithms (or ML libraries) produce better result with numerical variable. In python, library “sklearn” requires features in numerical arrays. Look at the below snapshot. I have applied random forest using sklearn library on titanic data set (only two features sex and pclass are taken as independent variables). It has returned an error because feature “sex” is categorical and has not been converted to numerical form.\n",
    "\n",
    "A common challenge with nominal categorical variable is that, it may decrease performance of a model. For example: We have two features “age” (range: 0-80) and “city” (81 different levels). Now, when we’ll apply label encoder to ‘city’ variable, it will represent ‘city’ with numeric values range from 0 to 80. The ‘city’ variable is now similar to ‘age’ variable since both will have similar data points, which is certainly not a right approach.\n",
    "\n",
    "Convert to number: As discussed above, some ML libraries do not take categorical variables as input. Thus, we convert them into numerical variables. Below are the methods to convert a categorical (string) input to numerical nature:\n",
    "Label Encoder: It is used to transform non-numerical labels to numerical labels (or nominal categorical variables). Numerical labels are always between 0 and n_classes-1. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "train[‘sex’] = number.fit_transform(train[‘sex’].astype(‘str’))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert numeric bins to number: Let’s say, bins of a continuous variable are available in the data set (shown below).\n",
    "\n",
    "Above, you can see that variable “Age” has bins (0-17, 17-25, 26-35 …). We can convert these bins into definite numbers using the following methods:\n",
    "Using label encoder for conversion. But, these numerical bins will be treated same as multiple levels of non-numeric feature. Hence, wouldn’t provide any additional information\n",
    "Create a new feature using mean or mode (most relevant value) of each age bucket. It would comprise of additional weight for levels.\n",
    "\n",
    "Create two new features, one for lower bound of age and another for upper bound. In this method, we’ll obtain more information about these numerical bins compare to earlier two methods.\n",
    "\n",
    "\n",
    "\n",
    "Combine levels: To avoid redundant levels in a categorical variable and to deal with rare levels, we can simply combine the different levels. There are various methods of combining levels. Here are commonly used ones:\n",
    "Using Business Logic: It is one of the most effective method of combining levels. It makes sense also to combine similar levels into similar groups based on domain or business experience. For example, we can combine levels of a variable “zip code” at state or district level. This will reduce the number of levels and improve the model performance also.\n",
    "\n",
    "Using frequency or response rate: Combining levels based on business logic is effective but we may always not have the domain knowledge. Imagine, you are given a data set from Aerospace Department, US Govt. How would you apply business logic here? In such cases, we combine levels by considering the frequency distribution or response rate.\n",
    "To combine levels using their frequency, we first look at the frequency distribution of of each level and combine levels having frequency less than 5% of total observation (5% is standard but you can change it based on distribution). This is an effective method to deal with rare levels.\n",
    "We can also combine levels by considering the response rate of each level. We can simply combine levels having similar response rate into same group.\n",
    "Finally, you can also look at both frequency and response rate to combine levels. You first combine levels based on response rate then combine rare levels to relevant group.\n",
    "\n",
    "Dummy Coding: Dummy coding is a commonly used method for converting a categorical input variable into continuous variable. ‘Dummy’, as the name suggests is a duplicate variable which represents one level of a categorical variable. Presence of a level is represent by 1 and absence is represented by 0. For every level present, one dummy variable will be created. Look at the representation below to convert a categorical variable using dummy variable.\n",
    "\n",
    "sex male and famale gets hot enocded above\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s come straight to the point on this one – there are only 2 types of variables you see – Continuous and Discrete. Further, discrete variables can divided into Nominal (categorical) and Ordinal.\n",
    "\n",
    "# Continuous\n",
    "\n",
    "Binning refers to dividing a list of continuous variables into groups. It is done to discover set of patterns in continuous variables, which are difficult to analyze otherwise. Also, bins are easy to analyze and interpret. But, it also leads to loss of information and loss of power. Once the bins are created, the information gets compressed into groups which later affects the final model. Hence, it is advisable to create small bins initially.\n",
    "\n",
    "This would help in minimal loss of information and produces better results. However, I’ve encountered cases where small bins doesn’t prove to be helpful. In such cases, you must decide for bin size according to your hypothesis.We should consider distribution of data prior to deciding bin size.\n",
    "\n",
    "In simpler words, it is a process of comparing variables at a ‘neutral’ or ‘standard’ scale. It helps to obtain same range of values. Normally distributed data is easy to read and interpret. As shown below, in a normally distributed data, 99.7% of the observations lie within 3 standard deviations from the mean. Also, the mean is zero and standard deviation is one. Normalization technique is commonly used in algorithms such as k-means, clustering etc.\n",
    "\n",
    "A commonly used normalization method is z-scores.\n",
    "\n",
    "Transformation is required when we encounter highly skewed data. It is suggested not to work on skewed data in its raw form. Because, it reduces the impact of low frequency values which could be equally significant. At times, skewness is influenced by presence of outliers. Hence, we need to be careful while using this approach. The technique to deal with outliers is explained in next sections.\n",
    "\n",
    "Business Logic adds precision to output of a model. Data alone can’t suggest you patterns which understanding its business can. Hence, in companies, data scientists often prefer to spend time with clients and understand their business and market. This not only helps them to make an informed decision. But, also enables them to think outside the data. Once you start thinking, you are no longer confined within data.\n",
    "\n",
    "Data are prone to outliers. Outlier is an abnormal value which stands apart from rest of data points. It can happen due to various reasons. Most common reason include challenges arising in data collection methods. Sometime the respondents deliberately provide incorrect answers; or the values are actually real. Then, how do we decide? You can any of these methods:\n",
    "\n",
    "Create a box plot. You’ll get Q1, Q2 and Q3. (data points > Q3 + 1.5IQR) and (data points < Q1 – 1.5IQR) will be considered as outliers. IQR is Interquartile Range. IQR = Q3-Q1\n",
    "Considering the scope of analysis, you can remove the top 1% and bottom 1% of values. However, this would result in loss of information. Hence, you must be check impact of these values on dependent variable.\n",
    "Treating outliers is a tricky situation – one where you need to combine business understanding and understanding of data. For example, if you are dealing with age of people and you see a value age = 200 (in years), the error is most likely happening because the data was collected incorrectly, or the person has entered age in months. Depending on what you think is likely, you would either remove (in case one) or replace by 200/12 years.\n",
    "\n",
    "Sometime data set has too many variables. May be, 100, 200 variables or even more. In such cases, you can’t build a model on all variables. Reason being, 1) It would be time consuming.  2) It might have lots of noise 3) A lot of variables will tell similar information\n",
    "\n",
    "Hence, to avoid such situation we use PCA a.k.a Principal Component Analysis. It is nothing but, finding out few ‘principal‘ variables which explain significant amount of variation in dependent variable. Using this technique, a large number of variables are reduced to few significant variables. This technique helps to reduce noise, redundancy and enables quick computations.\n",
    "\n",
    "In PCA, components are represented by PC1 or Comp 1, PC2 or Comp 2.. and so on. Here, PC1 will have highest variance followed by PC2, PC3 and so on. Our motive should be to select components with eigen values greater than 1. Eigen values are represented by ‘Standard Deviation’. Let check this out in R below:\n",
    "\n",
    "Create New Variables:\n",
    "\n",
    "Have a look at the date format above. I’m sure you can easily figure out the possible new variables. If you have still not figure out, no problem. Let me tell you. We can easily break the format in different variables namely:\n",
    "\n",
    "Date\n",
    "Month\n",
    "Year\n",
    "Time\n",
    "Days of Month\n",
    "Days of Week\n",
    "Days of Year\n",
    "I’ve listed down the possibilities. You aren’t required to create all the listed variables in every situation. Create only those variables which only sync with your hypothesis. Every variable would have an impact( high / low) on dependent variable. You can check it using correlation matrix.\n",
    "\n",
    "\n",
    "\n",
    "Create Bins:\n",
    "\n",
    "Once you have extracted new variables, you can now create bins. For example: You’ve ‘Months’ variable. You can easily create bins to obtain ‘quarter’, ‘half-yearly’ variables. In ‘Days’, you can create bins to obtain ‘weekdays’. Similarly, you’ll have to explore with these variables. Try and Repeat. Who knows, you might find a variable of highest importance.\n",
    "\n",
    "Convert Date to Numbers:\n",
    "\n",
    "You can also convert date to numbers and use them as numerical variables. This will allow you to analyze dates using various statistical techniques such as correlation. This would be difficult to undertake otherwise. On the basis of their response to dependent variable, you can then create their bins and capture another important trend in data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t•\tBagging: Known more formally as Bootstrapped Aggregation is where the same algorithm has different perspectives on the problem by being trained on different subsets of the training data.\n",
    "\t•\tBoosting: Different algorithms are trained on the same training data.\n",
    "\t•\tBlending: Known more formally as Stacked Aggregation or Stacking is where a variety of models whose predictions are taken as input to a new model that learns how to combine the predictions into an overall prediction.\n",
    "\t•\t\n",
    "\n",
    "How to Convert Categorical Data to Numerical Data?\n",
    "This involves two steps:\n",
    "\t1.\tInteger Encoding\n",
    "\t2.\tOne-Hot Encoding\n",
    "1. Integer Encoding\n",
    "As a first step, each unique category value is assigned an integer value.\n",
    "For example, “red” is 1, “green” is 2, and “blue” is 3.\n",
    "This is called a label encoding or an integer encoding and is easily reversible.\n",
    "For some variables, this may be enough.\n",
    "The integer values have a natural ordered relationship between each other and machine learning algorithms may be able to understand and harness this relationship.\n",
    "For example, ordinal variables like the “place” example above would be a good example where a label encoding would be sufficient.\n",
    "2. One-Hot Encoding\n",
    "For categorical variables where no such ordinal relationship exists, the integer encoding is not enough.\n",
    "In fact, using this encoding and allowing the model to assume a natural ordering between categories may result in poor performance or unexpected results (predictions halfway between categories).\n",
    "In this case, a one-hot encoding can be applied to the integer representation. This is where the integer encoded variable is removed and a new binary variable is added for each unique integer value.\n",
    "\n",
    "\n",
    "A one hot encoding is a representation of categorical variables as binary vectors.\n",
    "\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# define example\n",
    "data = ['cold', 'cold', 'warm', 'cold', 'hot', 'hot', 'warm', 'cold', 'warm', 'hot']\n",
    "values = array(data)\n",
    "print(values)\n",
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "print(integer_encoded)\n",
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "print(onehot_encoded)\n",
    "# invert first example\n",
    "inverted = label_encoder.inverse_transform([argmax(onehot_encoded[0, :])])\n",
    "print(inverted)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Missing values\n",
    "\n",
    "Occur during extraction, collection: missing at random, due to unobserved predictors, missing due to being missing: eg. People with higher incomes less likely to respond\n",
    "\n",
    "Deletion\n",
    "\n",
    "Medan/median/mode inputation\n",
    "Prediciton model: e.g. knn\n",
    "\n",
    "Outlier Detection and Treatment\n",
    "Outlier can be of two types: Univariate and Multivariate. Above, we have discussed the example of univariate outlier. These outliers can be found when we look at distribution of a single variable. Multi-variate outliers are outliers in an n-dimensional space. In order to find them, you have to look at distributions in multi-dimensions.\n",
    "\n",
    "\n",
    "\t1.\tArtificial (Error) / Non-natural\n",
    "\t2.\tNatural.\n",
    "\t3.\t\n",
    "How to detect Outliers?\n",
    "Most commonly used method to detect outliers is visualization. We use various visualization methods, like Box-plot, Histogram, Scatter Plot (above, we have used box plot and scatter plot for visualization). Some analysts also various thumb rules to detect outliers. Some of them are:\n",
    "\t•\tAny value, which is beyond the range of -1.5 x IQR to 1.5 x IQR\n",
    "\t•\tUse capping methods. Any value which out of range of 5th and 95th percentile can be considered as outlier\n",
    "\t•\tData points, three or more standard deviation away from mean are considered outlier\n",
    "\t•\tOutlier detection is merely a special case of the examination of data for influential data points and it also depends on the business understanding\n",
    "\t•\tBivariate and multivariate outliers are typically measured using either an index of influence or leverage, or distance. Popular indices such as Mahalanobis’ distance and Cook’s D are frequently used to detect outliers.\n",
    "\t•\tIn SAS, we can use PROC Univariate, PROC SGPLOT. To identify outliers and influential observation, we also look at statistical measure like STUDENT, COOKD, RSTUDENT and others.\n",
    "\n",
    "\n",
    "Most of the ways to deal with outliers are similar to the methods of missing values like deleting observations, transforming them, binning them, treat them as a separate group, imputing values and other statistical methods. Here, we will discuss the common techniques used to deal with outliers:\n",
    "Deleting observations: We delete outlier values if it is due to data entry error, data processing error or outlier observations are very small in numbers. We can also use trimming at both ends to remove outliers.\n",
    "Transforming and binning values: Transforming variables can also eliminate outliers. Natural log of a value reduces the variation caused by extreme values. Binning is also a form of variable transformation. Decision Tree algorithm allows to deal with outliers well due to binning of variable. We can also use the process of assigning weights to different observations.\n",
    "\n",
    "\n",
    "\n",
    "Imputing: Like imputation of missing values, we can also impute outliers. We can use mean, median, mode imputation methods. Before imputing values, we should analyse if it is natural outlier or artificial. If it is artificial, we can go with imputing values. We can also use statistical model to predict values of outlier observation and after that we can impute it with predicted values.\n",
    "Treat separately: If there are significant number of outliers, we should treat them separately in the statistical model. One of the approach is to treat both groups as two different groups and build individual model for both groups and then combine the output.\n",
    "\n",
    "\n",
    "What is the process of Feature Engineering ?\n",
    "You perform feature engineering once you have completed the first 5 steps in data exploration – Variable Identification, Univariate, Bivariate Analysis, Missing Values Imputation and Outliers Treatment. Feature engineering itself can be divided in 2 steps:\n",
    "\n",
    "\t•\tVariable transformation.\n",
    "\t•\tVariable / Feature creation.\n",
    "\n",
    "\n",
    "In data modelling, transformation refers to the replacement of a variable by a function. For instance, replacing a variable x by the square / cube root or logarithm x is a transformation. In other words, transformation is a process that changes the distribution or relationship of a variable with others.\n",
    "\n",
    "In data modelling, transformation refers to the replacement of a variable by a function. For instance, replacing a variable x by the square / cube root or logarithm x is a transformation. In other words, transformation is a process that changes the distribution or relationship of a variable with others.\n",
    "Below are the situations where variable transformation is a requisite:\n",
    "\t•\tWhen we want to change the scale of a variable or standardize the values of a variable for better understanding. While this transformation is a must if you have data in different scales, this transformation does not change the shape of the variable distribution\n",
    "\t•\tWhen we can transform complex non-linear relationships into linear relationships. Existence of a linear relationship between variables is easier to comprehend compared to a non-linear or curved relation. Transformation helps us to convert a non-linear relation into linear relation. Scatter plot can be used to find the relationship between two continuous variables. These transformations also improve the prediction. Log transformation is one of the commonly used transformation technique used in these situations.\n",
    "\n",
    "\n",
    "Symmetric distribution is preferred over skewed distribution as it is easier to interpret and generate inferences. Some modeling techniques requires normal distribution of variables. So, whenever we have a skewed distribution, we can use transformations which reduce skewness. For right skewed distribution, we take square / cube root or logarithm of variable and for left skewed, we take square / cube or exponential of variables.\n",
    "\n",
    "\t•\tVariable Transformation is also done from an implementation point of view (Human involvement). Let’s understand it more clearly. In one of my project on employee performance, I found that age has direct correlation with performance of the employee i.e. higher the age, better the performance. From an implementation stand point, launching age based progamme might present implementation challenge. However, categorizing the sales agents in three age group buckets of <30 years, 30-45 years and >45  and then formulating three different strategies for each group is a judicious approach. This categorization technique is known as Binning of Variables.\n",
    "\n",
    "\n",
    "There are various methods used to transform variables. As discussed, some of them include square root, cube root, logarithmic, binning, reciprocal and many others. Let’s look at these methods in detail by highlighting the pros and cons of these transformation methods.\n",
    "\t•\tLogarithm: Log of a variable is a common transformation method used to change the shape of distribution of the variable on a distribution plot. It is generally used for reducing right skewness of variables. Though, It can’t be applied to zero or negative values as well.\n",
    "\t•\tSquare / Cube root: The square and cube root of a variable has a sound effect on variable distribution. However, it is not as significant as logarithmic transformation. Cube root has its own advantage. It can be applied to negative values including zero. Square root can be applied to positive values including zero.\n",
    "\t•\tBinning: It is used to categorize variables. It is performed on original values, percentile or frequency. Decision of categorization technique is based on business understanding. For example, we can categorize income in three categories, namely: High, Average and Low. We can also perform co-variate binning which depends on the value of more than one variables.\n",
    "\n",
    "\n",
    "Feature / Variable creation is a process to generate a new variables / features based on existing variable(s). For example, say, we have date(dd-mm-yy) as an input variable in a data set. We can generate new variables like day, month, year, week, weekday that may have better relationship with target variable. This step is used to highlight the hidden relationship in a variable:\n",
    "\n",
    "\t•\tCreating derived variables: This refers to creating new variables from existing variable(s) using set of functions or different methods. Let’s look at it through “Titanic – Kaggle competition”. In this data set, variable age has missing values. To predict missing values, we used the salutation (Master, Mr, Miss, Mrs) of name as a new variable. How do we decide which variable to create? Honestly, this depends on business understanding of the analyst, his curiosity and the set of hypothesis he might have about the problem. Methods such as taking log of variables, binning variables and other methods of variable transformation can also be used to create new variables.\n",
    "\t•\tCreating dummy variables: One of the most common application of dummy variable is to convert categorical variable into numerical variables. Dummy variables are also called Indicator Variables. It is useful to take categorical variable as a predictor in statistical models.  Categorical variable can take values 0 and 1. Let’s take a variable ‘gender’. We can produce two variables, namely, “Var_Male” with values 1 (Male) and 0 (No male) and “Var_Female” with values 1 (Female) and 0 (No Female). We can also create dummy variables for more than two classes of a categorical variables with n or n-1 dummy variables.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Univariate\n",
    "\n",
    "Continuous Variables:- In case of continuous variables, we need to understand the central tendency and spread of the variable. These are measured using various statistical metrics visualization methods as shown below:\n",
    "\n",
    "Categorical Variables:- For categorical variables, we’ll use frequency table to understand distribution of each category. We can also read as percentage of values under each category. It can be be measured using two metrics, Count and Count% against each category. Bar chart can be used as visualization.\n",
    "\n",
    "Bi-variate Analysis\n",
    "\n",
    "Continuous & Continuous: While doing bi-variate analysis between two continuous variables, we should look at scatter plot. It is a nifty way to find out the relationship between two variables. The pattern of scatter plot indicates the relationship between variables. The relationship can be linear or non-linear.\n",
    "\n",
    "Categorical & Categorical: To find the relationship between two categorical variables, we can use following methods:\n",
    "\t•\tTwo-way table: We can start analyzing the relationship by creating a two-way table of count and count%. The rows represents the category of one variable and the columns represent the categories of the other variable. We show count or count% of observations available in each combination of row and column categories.\n",
    "\t•\tStacked Column Chart: This method is more of a visual form of Two-way table.\n",
    "\t•\t\n",
    "\n",
    "\t•\tChi-Square Test: This test is used to derive the statistical significance of relationship between the variables. Also, it tests whether the evidence in the sample is strong enough to generalize that the relationship for a larger population as well. Chi-square is based on the difference between the expected and observed frequencies in one or more categories in the two-way table. It returns probability for the computed chi-square distribution with the degree of freedom.\n",
    "\n",
    "\n",
    "Categorical & Continuous: While exploring relation between categorical and continuous variables, we can draw box plots for each level of categorical variables. If levels are small in number, it will not show the statistical significance. To look at the statistical significance we can perform Z-test, T-test or ANOVA.\n",
    "\t•\tZ-Test/ T-Test:- Either test assess whether mean of two groups are statistically different from each other or not.\n",
    "\n",
    "\n",
    "\t•\tANOVA:- It assesses whether the average of more than two groups is statistically different.\n",
    "\n",
    "\n",
    "Example: Suppose, we want to test the effect of five different exercises. For this, we recruit 20 men and assign one type of exercise to 4 men (5 groups). Their weights are recorded after a few weeks. We need to find out whether the effect of these exercises on them is significantly different or not. This can be done by comparing the weights of the 5 groups of 4 men each.\n",
    "Till here, we have understood the first three stages of Data Exploration, Variable Identification, Uni-Variate and Bi-Variate analysis. We also looked at various statistical and visual methods to identify the relationship between variables. \n",
    "\n",
    "Now, we will look at the methods of Missing values Treatment. More importantly, we will also look at why missing values occur in our data and why treating them is necessary.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
